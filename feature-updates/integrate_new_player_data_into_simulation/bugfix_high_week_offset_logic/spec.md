# Bug Fix Spec: Week Offset Logic

**Created:** 2026-01-02
**Priority:** HIGH
**Status:** Stage 2 (Deep Dive)

---

## Executive Summary

**The Bug:** Accuracy Simulation loads the WRONG week folder for actual points, resulting in all 0.0 values and completely non-functional feature.

**Root Cause:** Misinterpreted epic notes as "week 17 special case" instead of "ALL weeks use week_N + week_N+1 pattern", combined with no hands-on data inspection to verify assumptions.

**Impact:** Feature produces garbage output (MAE calculated with all zeros), would be useless in production.

**Solution:** Update `_load_season_data()` to return (week_N, week_N+1) folders, use projected data from week_N and actual data from week_N+1.

---

## Epic Requirement (Direct Quote)

**Source:** `integrate_new_player_data_into_simulation_notes.txt` line 8

> "Additionally, I want to verify if Week 17 is being correctly assessed in both sims. When running score_player calculations, it should use the week_17 folders to determine a projected_points of the player, then it should look at the actual_points array in week_18 folders to determine what the player actually scored in week 17"

**Interpretation:**
- Use week_17 folder for projected_points
- Use week_18 folder for actual_points
- Example uses week 17, but pattern applies to ALL weeks
- **For week N:** Use week_N for projections, week_N+1 for actuals

---

## Data Model Investigation (Evidence-Based)

### Assumption Validation Table

| Assumption | How Verified | Evidence |
|------------|--------------|----------|
| week_N folder contains week N actuals | Manual data inspection | week_01/qb_data.json: actual_points[0] = 0.0 ‚úó |
| week_N+1 folder contains week N actuals | Manual data inspection | week_02/qb_data.json: actual_points[0] = 33.6 ‚úì |
| Data is point-in-time (as of week N start) | Code investigation | json_exporter.py:306 - "if week < current_week" |
| Arrays have 17 elements | Manual data inspection | len(actual_points) = 17 ‚úì |
| Week 17 uses week_18 for actuals | Epic notes + data model | Pattern applies to ALL weeks |

### Manual Data Inspection Results

**Command executed (Stage 5a.5 principle - hands-on inspection):**
```python
import json

# Load week_01 QB data
with open('simulation/sim_data/2021/weeks/week_01/qb_data.json') as f:
    week_01 = json.load(f)

# Load week_02 QB data
with open('simulation/sim_data/2021/weeks/week_02/qb_data.json') as f:
    week_02 = json.load(f)

print(f'Week 1 actuals in week_01 folder: {week_01[0]["actual_points"][0]}')
print(f'Week 1 actuals in week_02 folder: {week_02[0]["actual_points"][0]}')
```

**Actual Output:**
```
Week 1 actuals in week_01 folder: 0.0
Week 1 actuals in week_02 folder: 33.6
```

**Analysis:**
- week_01 folder (Josh Allen): actual_points[0] = 0.0 (week 1 not complete yet)
- week_02 folder (Kyler Murray): actual_points[0] = 33.6 (week 1 now complete)
- **Conclusion:** week_N folder does NOT contain week N actuals

### Data Generation Logic Investigation

**Source:** `historical_data_compiler/json_exporter.py` lines 303-312

```python
# Build actual_points array (actuals for weeks 1 to N-1, 0.0 for N to 17)
actual_points = []
for week in range(1, REGULAR_SEASON_WEEKS + 1):
    if week < current_week:
        # Past weeks: use actual points
        points = player_data.week_points.get(week, 0.0)
        actual_points.append(points if points else 0.0)
    else:
        # Current and future weeks: 0.0
        actual_points.append(0.0)
```

**Analysis:**
- `current_week = N` (the week folder being generated)
- `if week < current_week:` means weeks 1 to N-1 have actual values
- `else:` means weeks N to 17 have 0.0 (not yet completed)
- **Conclusion:** week_N folder contains actuals for weeks 1 to N-1 ONLY

**Data Model Diagram:**
```
week_01 folder (current_week=1):
  actual_points = [0.0, 0.0, 0.0, ... 0.0]  # Week 1-17 all zeros (none complete)

week_02 folder (current_week=2):
  actual_points = [33.6, 0.0, 0.0, ... 0.0]  # Week 1 complete, 2-17 zeros

week_03 folder (current_week=3):
  actual_points = [33.6, 28.4, 0.0, ... 0.0]  # Week 1-2 complete, 3-17 zeros

week_18 folder (current_week=18):
  actual_points = [33.6, 28.4, ... 24.2]  # Week 1-17 all complete
```

**This is a point-in-time data structure** representing data "as of" the start of week N.

---

## Current Implementation Analysis (Broken Code)

### Bug Location 1: _load_season_data() Returns Same Folder Twice

**File:** `simulation/accuracy/AccuracySimulationManager.py`
**Lines:** 293-313

**Current Code (BROKEN):**
```python
def _load_season_data(
    self,
    season_path: Path,
    week_num: int
) -> Tuple[Optional[Path], Optional[Path]]:
    """
    Load data paths for a specific week in a season.

    Args:
        season_path: Path to season folder (e.g., sim_data/2024/)
        week_num: Week number (1-17)

    Returns:
        Tuple of (week_folder, week_folder) or (None, None) if week folder not found
    """
    week_folder = season_path / "weeks" / f"week_{week_num:02d}"

    if not week_folder.exists():
        return None, None

    return week_folder, week_folder  # ‚ùå BUG: Returns same folder twice!
```

**What's Wrong:**
- Line 313: `return week_folder, week_folder`
- Returns the SAME folder for both projected_path and actual_path
- Should return (week_N, week_N+1)

### Bug Location 2: get_accuracy_for_week() Uses Wrong Data

**File:** `simulation/accuracy/AccuracySimulationManager.py`
**Lines:** 411-456

**Current Code (BROKEN):**
```python
for week_num in range(start_week, end_week + 1):
    projected_path, actual_path = self._load_season_data(season_path, week_num)
    # ‚ùå Both paths point to week_num folder

    if not projected_path:
        continue

    # Create player manager with this config
    player_mgr = self._create_player_manager(config_dict, projected_path, season_path)
    # ‚ùå Only uses projected_path, ignores actual_path

    # ... scoring logic ...

    # Get actual points for this specific week (from actual_points array)
    if 1 <= week_num <= 17 and len(player.actual_points) >= week_num:
        actual = player.actual_points[week_num - 1]
        # ‚ùå Gets actual_points[week_num-1] from week_num folder
        # ‚ùå week_num folder has 0.0 for week_num actuals!
        if actual is not None and actual > 0:
            actuals[player.id] = actual
```

**What's Wrong:**
1. Line 412: `_load_season_data()` returns (week_N, week_N) instead of (week_N, week_N+1)
2. Line 417: Only uses `projected_path`, ignores `actual_path`
3. Lines 453-456: Gets `actual_points[week_num-1]` from player loaded from week_num folder
4. **Result:** Always gets 0.0 for actual points (week hasn't completed yet in week_num folder)

### Bug Location 3: ParallelAccuracyRunner Has Same Bug

**File:** `simulation/accuracy/ParallelAccuracyRunner.py`
**Similar Issue:** Same pattern as AccuracySimulationManager

**Note:** Both files need identical fixes to maintain consistency.

---

## Solution Design

### High-Level Approach

**For week N accuracy calculation:**
1. Load week_N folder for **projections**
2. Load week_N+1 folder for **actuals**
3. Use projected_points from week_N
4. Use actual_points[N-1] from week_N+1

**Special Case - Week 17:**
- Projections from week_17 folder
- Actuals from week_18 folder
- Same pattern as all other weeks

### Code Changes Required

#### Change 1: Update _load_season_data() Method

**File:** `simulation/accuracy/AccuracySimulationManager.py` (lines 293-313)

**Current (BROKEN):**
```python
def _load_season_data(
    self,
    season_path: Path,
    week_num: int
) -> Tuple[Optional[Path], Optional[Path]]:
    """
    Load data paths for a specific week in a season.

    Returns:
        Tuple of (week_folder, week_folder) or (None, None) if week folder not found
    """
    week_folder = season_path / "weeks" / f"week_{week_num:02d}"

    if not week_folder.exists():
        return None, None

    return week_folder, week_folder  # ‚ùå BUG
```

**Correct (FIXED):**
```python
def _load_season_data(
    self,
    season_path: Path,
    week_num: int
) -> Tuple[Optional[Path], Optional[Path]]:
    """
    Load data paths for a specific week in a season.

    For accuracy calculations, we need TWO week folders:
    - week_N folder: Contains projected_points for week N
    - week_N+1 folder: Contains actual_points for week N

    This is because week_N folder represents data "as of" week N's start,
    so week N's actual results aren't known until week N+1.

    Args:
        season_path: Path to season folder (e.g., sim_data/2024/)
        week_num: Week number (1-17)

    Returns:
        Tuple of (projected_folder, actual_folder) or (None, None) if folders not found
        - projected_folder: week_N folder (for projected_points)
        - actual_folder: week_N+1 folder (for actual_points)
    """
    # Week N folder for projections
    projected_folder = season_path / "weeks" / f"week_{week_num:02d}"

    # Week N+1 folder for actuals
    # For week 1: use week_02, for week 17: use week_18
    actual_week_num = week_num + 1
    actual_folder = season_path / "weeks" / f"week_{actual_week_num:02d}"

    # Both folders must exist
    if not projected_folder.exists():
        self.logger.warning(f"Projected folder not found: {projected_folder}")
        return None, None

    if not actual_folder.exists():
        self.logger.warning(
            f"Actual folder not found: {actual_folder} "
            f"(needed for week {week_num} actuals)"
        )
        return None, None

    return projected_folder, actual_folder  # ‚úì FIXED
```

**Changes:**
1. Create TWO folder paths: `projected_folder` (week_N) and `actual_folder` (week_N+1)
2. Check both folders exist
3. Return (projected_folder, actual_folder) instead of (week_folder, week_folder)
4. Update docstring to explain WHY we need two folders

#### Change 2: Update get_accuracy_for_week() to Use Both Folders

**File:** `simulation/accuracy/AccuracySimulationManager.py` (lines 411-456)

**Current (BROKEN):**
```python
for week_num in range(start_week, end_week + 1):
    projected_path, actual_path = self._load_season_data(season_path, week_num)
    if not projected_path:
        continue

    # Create player manager with this config
    player_mgr = self._create_player_manager(config_dict, projected_path, season_path)
    # ‚ùå Only uses projected_path

    # ... scoring ...

    # Get actual points from player loaded from projected_path
    if 1 <= week_num <= 17 and len(player.actual_points) >= week_num:
        actual = player.actual_points[week_num - 1]  # ‚ùå Gets 0.0
```

**Correct (FIXED):**
```python
for week_num in range(start_week, end_week + 1):
    projected_path, actual_path = self._load_season_data(season_path, week_num)
    if not projected_path or not actual_path:
        # Skip if either folder missing
        continue

    # Create TWO player managers:
    # 1. From projected_path (week_N) for projections
    # 2. From actual_path (week_N+1) for actuals

    projected_mgr = self._create_player_manager(config_dict, projected_path, season_path)
    actual_mgr = self._create_player_manager(config_dict, actual_path, season_path)

    try:
        projections = {}
        actuals = {}
        player_data = []

        # Calculate max weekly projection from projected folder
        max_weekly = projected_mgr.calculate_max_weekly_projection(week_num)
        projected_mgr.scoring_calculator.max_weekly_projection = max_weekly

        # Get projections from week_N folder
        for player in projected_mgr.players:
            scored = projected_mgr.score_player(
                player,
                use_weekly_projection=True,
                adp=False,
                player_rating=False,
                team_quality=True,
                performance=True,
                matchup=True,
                schedule=False,
                bye=False,
                injury=False,
                temperature=True,
                wind=True,
                location=True
            )
            if scored:
                projections[player.id] = scored.projected_points

        # Get actuals from week_N+1 folder
        # Use actual_mgr.players (loaded from week_N+1 folder)
        for player in actual_mgr.players:
            # Get week N actuals from week_N+1 folder
            # week_N+1 has actual_points[N-1] populated (week N complete)
            if 1 <= week_num <= 17 and len(player.actual_points) >= week_num:
                actual = player.actual_points[week_num - 1]  # ‚úì Gets real value from week_N+1
                if actual is not None and actual > 0:
                    actuals[player.id] = actual

                    # Match with projection by player ID
                    if player.id in projections:
                        player_data.append({
                            'name': player.name,
                            'position': player.position,
                            'projected': projections[player.id],
                            'actual': actual
                        })

        week_projections[week_num] = projections
        week_actuals[week_num] = actuals
        player_data_by_week[week_num] = player_data

    finally:
        self._cleanup_player_manager(projected_mgr)
        self._cleanup_player_manager(actual_mgr)
```

**Changes:**
1. Create TWO player managers: `projected_mgr` and `actual_mgr`
2. Get projections from `projected_mgr.players` (week_N folder)
3. Get actuals from `actual_mgr.players` (week_N+1 folder)
4. Match by player ID (projections[player.id] and actuals[player.id])
5. Cleanup both managers

**Alternative Simpler Approach (if player IDs are stable):**

If we can guarantee player IDs don't change between week_N and week_N+1, we could simplify by loading players from actual_path only and using projected_points from week_N via PlayerManager's caching:

```python
# Simpler approach (if feasible):
# Load from actual_path only, but tell PlayerManager to use week_N for projections
player_mgr = self._create_player_manager(config_dict, actual_path, season_path)

# player.projected_points would come from week_N projection logic
# player.actual_points[week_num-1] would come from week_N+1 folder
```

**Decision needed:** Which approach is more reliable given PlayerManager's implementation?

#### Change 3: Apply Same Fix to ParallelAccuracyRunner

**File:** `simulation/accuracy/ParallelAccuracyRunner.py`

**Same changes as AccuracySimulationManager:**
1. Update `_load_season_data()` method (same logic)
2. Update week calculation loop (same logic)
3. Maintain consistency between serial and parallel implementations

---

## Testing Strategy

### Unit Tests Required

**Test File:** `tests/simulation/accuracy/test_accuracy_simulation_manager.py`

1. **test_load_season_data_returns_two_folders()**
   - Call `_load_season_data(season_path, 1)`
   - Assert returns (week_01, week_02)
   - Call `_load_season_data(season_path, 17)`
   - Assert returns (week_17, week_18)

2. **test_load_season_data_handles_missing_actual_folder()**
   - Call `_load_season_data(season_path, 18)`
   - Assert returns (None, None) (week_19 doesn't exist)

3. **test_get_accuracy_uses_correct_folders()**
   - Run accuracy for week 1
   - Mock both player managers, verify:
     - projected_mgr created from week_01
     - actual_mgr created from week_02
   - Verify actual_points[0] from actual_mgr is non-zero

### Integration Tests Required

**Test File:** `tests/simulation/accuracy/integration/test_accuracy_end_to_end.py`

1. **test_week_1_accuracy_with_real_data()**
   - Run accuracy calculation for week 1
   - Verify actual_points are non-zero (not all 0.0)
   - Verify MAE is realistic (3-8 range for QB)

2. **test_week_17_uses_week_18_folder()**
   - Run accuracy calculation for week 17
   - Verify uses week_18 folder for actuals
   - Verify week 17 actuals are non-zero

3. **test_all_weeks_have_realistic_mae()**
   - Run accuracy for weeks 1-17
   - For EACH week, verify:
     - >50% of players have non-zero actuals
     - MAE is in realistic range (2-10)
     - Variance > 0 (not all identical)

### Smoke Testing (Stage 5ca - Enhanced)

**Part 3: E2E Execution Test with Statistical Sanity Checks**

```python
import json
import statistics

# Load week_01 and week_02 to verify data source
week_01 = json.load(open('simulation/sim_data/2025/weeks/week_01/qb_data.json'))
week_02 = json.load(open('simulation/sim_data/2025/weeks/week_02/qb_data.json'))

print("=== DATA SOURCE VERIFICATION ===")
print(f"Week 1 QB in week_01 folder - actual_points[0]: {week_01[0]['actual_points'][0]}")
print(f"Week 1 QB in week_02 folder - actual_points[0]: {week_02[0]['actual_points'][0]}")

if week_01[0]['actual_points'][0] == 0.0 and week_02[0]['actual_points'][0] > 0:
    print("[PASS] Data model confirmed: week_01 has 0.0, week_02 has real values")
else:
    print("[FAIL] Data model unexpected")
    sys.exit(1)

# Run accuracy calculation
result = run_accuracy_simulation(season=2025, start_week=1, end_week=1)

# Extract actual points used
actuals = result.week_actuals[1].values()

print("\\n=== STATISTICAL SANITY CHECKS ===")

# Check 1: Zero percentage
zero_count = sum(1 for v in actuals if v == 0.0)
zero_pct = (zero_count / len(actuals)) * 100 if len(actuals) > 0 else 100

print(f"Zero percentage: {zero_pct:.1f}%")
if zero_pct > 90:
    print("[FAIL] üö® >90% zeros - THIS IS A BUG (week offset logic broken)")
    sys.exit(1)
elif zero_pct > 50:
    print("[WARNING] >50% zeros - investigate")
else:
    print("[PASS] Zero percentage acceptable")

# Check 2: Variance
if len(actuals) > 1:
    stddev = statistics.stdev(actuals)
    print(f"Standard deviation: {stddev:.2f}")
    if stddev == 0:
        print("[FAIL] üö® All values identical - THIS IS A BUG")
        sys.exit(1)
    else:
        print("[PASS] Values have variance")

# Check 3: Realistic MAE range
mae = result.overall_mae
print(f"MAE: {mae:.2f}")
if mae < 1.0:
    print("[WARNING] MAE suspiciously low - projections too perfect")
elif mae > 15.0:
    print("[WARNING] MAE suspiciously high - projections terrible")
elif 3.0 <= mae <= 8.0:
    print("[PASS] MAE in realistic NFL range")
else:
    print("[PASS] MAE outside typical range but not impossible")

# Check 4: Non-zero count
non_zero = len(actuals) - zero_count
print(f"\\nPlayers with non-zero actuals: {non_zero}/{len(actuals)} ({100-zero_pct:.1f}%)")
if non_zero == 0:
    print("[FAIL] üö® ZERO players have non-zero actuals - THIS IS A BUG")
    sys.exit(1)
else:
    print("[PASS] Some players have non-zero actuals")

print("\\n[SUCCESS] All statistical sanity checks passed")
```

**Critical Question Checklist:**
- [ ] Are the output values statistically realistic?
- [ ] If I saw "(0 have non-zero points)" would I mark it PASS? (Answer: NO - AUTOMATIC FAIL)
- [ ] Does MAE fall in expected range for NFL data?
- [ ] Do values have variance, or are they all identical?
- [ ] If this ran in production, would users trust the results?

---

## Verification Plan

### Pre-Implementation Verification (Stage 5a.5)

**Hands-On Data Inspection (MANDATORY):**

```python
# Open Python REPL BEFORE implementing
import json

# Load week_01, week_02, week_18
week_01 = json.load(open('simulation/sim_data/2021/weeks/week_01/qb_data.json'))
week_02 = json.load(open('simulation/sim_data/2021/weeks/week_02/qb_data.json'))
week_18 = json.load(open('simulation/sim_data/2021/weeks/week_18/qb_data.json'))

# Verify week 1 actuals
print("Week 1 actuals:")
print(f"  week_01 folder: {week_01[0]['actual_points'][0]}")  # Expect 0.0
print(f"  week_02 folder: {week_02[0]['actual_points'][0]}")  # Expect non-zero

# Verify week 17 actuals
print("\\nWeek 17 actuals:")
print(f"  week_17 folder: {week_17[0]['actual_points'][16]}")  # Expect 0.0
print(f"  week_18 folder: {week_18[0]['actual_points'][16]}")  # Expect non-zero

# Verify array lengths
print(f"\\nArray lengths: {len(week_01[0]['actual_points'])}")  # Expect 17
```

**Expected Output:**
```
Week 1 actuals:
  week_01 folder: 0.0
  week_02 folder: 33.6

Week 17 actuals:
  week_17 folder: 0.0
  week_18 folder: 24.2

Array lengths: 17
```

**If this doesn't match expectations ‚Üí DO NOT IMPLEMENT (investigate data model further)**

### Post-Implementation Verification

**After implementing fix:**

1. **Re-run hands-on data inspection** - verify understanding still correct
2. **Run unit tests** - all must pass
3. **Run integration tests** - verify realistic MAE values
4. **Run smoke tests** - with statistical sanity checks
5. **Verify week 17 specifically** - uses week_18 folder

### Cross-Epic Verification (User Requirement)

**User specified:** "verify ALL changes made in this epic against the notes and original documentation/code of the simulations"

**Verification checklist:**

1. **Compare with Feature 01 (Win Rate Sim):**
   - Does Win Rate Sim have similar week offset logic?
   - Do both features use consistent patterns?
   - Any unintended interactions?

2. **Verify against original epic notes:**
   - [ ] Line 1: Update to use JSON files (not CSV) ‚úì
   - [ ] Line 3-6: Load JSON from week folders ‚úì
   - [ ] Line 6: Handle drafted_by, locked, projected_points, actual_points ‚úì
   - [ ] Line 8: Week 17 uses week_18 for actuals ‚úì (NOW FIXED)

3. **Verify against original simulation code (pre-epic):**
   - Read pre-epic AccuracySimulationManager.py (before JSON changes)
   - Identify what calculation logic should remain unchanged
   - Verify only data loading changed, not algorithms

4. **Cross-reference with original CSV implementation:**
   - How did CSV version handle this?
   - Did CSV have separate files for projected vs actual?
   - Are we maintaining equivalent functionality?

---

## Risk Analysis

### Risk 1: Player ID Mismatch Between Folders

**Risk:** Player IDs might differ between week_N and week_N+1 folders

**Mitigation:**
- Match by player ID (not array index)
- Log warning if player exists in one folder but not the other
- Test with actual data to verify IDs are stable

### Risk 2: Week 18 Folder Missing

**Risk:** Some seasons might not have week_18 folder (no week 17 actuals available)

**Mitigation:**
- `_load_season_data()` already checks folder existence
- Returns (None, None) if actual_folder missing
- Week 17 will be skipped if week_18 doesn't exist (acceptable)

### Risk 3: Performance Impact

**Risk:** Loading two PlayerManager instances per week doubles overhead

**Mitigation:**
- Profile performance before/after
- If significant impact, investigate caching or shared loading
- For accuracy sim (run infrequently), acceptable tradeoff for correctness

### Risk 4: Interaction with Feature 01

**Risk:** Changes to AccuracySimulationManager might affect WinRateSimulationManager

**Mitigation:**
- Feature 01 and Feature 02 use separate files (no shared code)
- WinRateSimulationManager doesn't need week offset (uses current week only)
- Verify Feature 01 tests still pass after bug fix

---

## Implementation Checklist

**Phase 1: Code Changes**
- [ ] Update `AccuracySimulationManager._load_season_data()` (return two folders)
- [ ] Update `AccuracySimulationManager.get_accuracy_for_week()` (use both folders)
- [ ] Update `ParallelAccuracyRunner._load_season_data()` (same changes)
- [ ] Update `ParallelAccuracyRunner` week calculation (same changes)
- [ ] Update docstrings (explain why two folders needed)

**Phase 2: Testing**
- [ ] Add unit test: `test_load_season_data_returns_two_folders()`
- [ ] Add unit test: `test_load_season_data_handles_missing_actual_folder()`
- [ ] Add unit test: `test_get_accuracy_uses_correct_folders()`
- [ ] Add integration test: `test_week_1_accuracy_with_real_data()`
- [ ] Add integration test: `test_week_17_uses_week_18_folder()`
- [ ] Add integration test: `test_all_weeks_have_realistic_mae()`
- [ ] Run all existing tests (2463 must pass)

**Phase 3: Smoke Testing (Enhanced)**
- [ ] Part 1: Import test (imports don't crash)
- [ ] Part 2: Entry point test (N/A - no entry point changes)
- [ ] Part 3: E2E test with statistical sanity checks:
  - [ ] Verify week_01 has 0.0, week_02 has non-zero (data source)
  - [ ] Zero percentage <90% (AUTOMATIC FAIL if >=90%)
  - [ ] Variance > 0 (not all identical)
  - [ ] MAE in realistic range (2-10 for NFL)
  - [ ] >0 players with non-zero actuals

**Phase 4: Cross-Epic Verification**
- [ ] Verify Feature 01 tests still pass
- [ ] Compare with epic notes (all requirements met?)
- [ ] Compare with original simulation code (only data loading changed?)
- [ ] Verify week 17 specifically uses week_18

**Phase 5: Documentation**
- [ ] Update code_changes.md
- [ ] Update implementation_checklist.md
- [ ] Update lessons_learned.md with final insights

---

## Lessons Learned Integration

This bug fix implements ALL 6 prevention strategies from `lessons_learned.md`:

**Strategy 1: Stage 2.5 Principles (Spec Validation)**
- ‚úì Re-read epic notes with fresh eyes (line 8 analyzed)
- ‚úì Validated EVERY claim against actual code/data
- ‚úì Documented evidence (code line numbers, actual data values)
- ‚úì Created Assumption Validation Table

**Strategy 2: Stage 5a.5 Principles (Hands-On Data Inspection)**
- ‚úì Manual data inspection BEFORE implementing (Python REPL)
- ‚úì Printed actual values (not just "exists" checks)
- ‚úì Verified week_01 has 0.0, week_02 has 33.6
- ‚úì Investigated data generation logic (json_exporter.py)

**Strategy 3: Data Sanity Checks (Smoke Testing)**
- ‚úì Statistical validation (zero percentage, variance, ranges)
- ‚úì Critical question: "Would I be suspicious in production?"
- ‚úì EXPLICIT RULE: "(0 have non-zero)" ‚Üí AUTOMATIC FAIL

**Strategy 4: Statistical Validation (QC Round 2)**
- ‚úì Will verify MAE in realistic range (3-8 for NFL)
- ‚úì Will check output values, not just structure

**Strategy 5: Spec Re-Validation (Iteration 25)**
- ‚úì Re-read epic notes during spec creation
- ‚úì Verified spec matches epic EXACTLY (not interpretation)

**Strategy 6: Critical Questions Checklists**
- ‚úì Added to verification plan
- ‚úì Will use during smoke testing and QC rounds

---

## Acceptance Criteria

**This bug fix is complete when:**

1. **Code Changes:**
   - [ ] `_load_season_data()` returns (week_N, week_N+1)
   - [ ] `get_accuracy_for_week()` uses both folders correctly
   - [ ] ParallelAccuracyRunner has same changes

2. **Testing:**
   - [ ] All unit tests pass (including new tests)
   - [ ] All integration tests pass
   - [ ] Full test suite passes (2463/2463)

3. **Smoke Testing:**
   - [ ] Part 3 E2E test shows >0% non-zero actuals (NOT "0 have non-zero")
   - [ ] MAE in realistic range (3-8)
   - [ ] Statistical sanity checks pass

4. **QC Rounds:**
   - [ ] Round 1 passes (Basic Validation)
   - [ ] Round 2 passes (Deep Verification + Output Validation)
   - [ ] Round 3 passes (Skeptical Review + Data Source Validation)

5. **Cross-Epic Verification:**
   - [ ] Feature 01 tests still pass
   - [ ] Epic notes requirements all met
   - [ ] Original simulation algorithms unchanged

6. **Documentation:**
   - [ ] code_changes.md updated
   - [ ] lessons_learned.md updated
   - [ ] All verification checklists complete

**User approval required before bug fix considered complete.**

---

*This spec implements ALL lessons learned from the catastrophic "0.0 acceptance" failure. Every claim is validated against actual code or data. No assumptions without evidence.*
