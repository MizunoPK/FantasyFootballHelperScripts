BUG FIX: Week Offset Logic
Priority: HIGH
Discovered: 2026-01-02
Discovered During: Feature 02 - Stage 5cc (Final Review) - caught by user questioning

----

ISSUE DESCRIPTION:

Accuracy Simulation feature loads the WRONG week folder for actual points, resulting in all 0.0 values and completely non-functional accuracy calculations.

What's wrong:
- Code loads week_N folder for BOTH projected and actual points
- Week_N folder contains actual_points[N-1] = 0.0 (week N hasn't completed yet)
- Should load week_N for projected, week_N+1 for actual
- Result: All accuracy calculations use 0.0 for actuals → meaningless MAE values

How discovered:
- User asked: "When calculating scores for week X, does it correctly use projected points from Week X's file, and actual points from Week X+1's file?"
- Investigation revealed code loads single week folder (week_N) for both
- Data inspection showed week_01 has actual_points[0]=0.0, week_02 has actual_points[0]=33.6
- Epic notes line 8 explicitly stated: "use week_17 folders for projected_points, then look at actual_points array in week_18 folders"

Impact:
- Feature is COMPLETELY NON-FUNCTIONAL
- All accuracy calculations produce garbage output (comparing projections to zeros)
- Would produce MAE = NaN or meaningless values in production
- Epic requirement explicitly stated this pattern, but ALL 7 workflow stages failed to catch it:
  - Stage 2: Misinterpreted epic as "week 17 special case only"
  - Stage 5a: All 24 iterations trusted bad spec
  - Stage 5b: No hands-on data inspection before coding
  - Stage 5ca: CATASTROPHIC - accepted "(0 have non-zero points)" as PASS
  - Stage 5cb R1-3: Checked data structure, not data semantics
  - User caught it immediately by asking basic verification question

----

ROOT CAUSE:

**Reading Comprehension Failure (Stage 2):**
- Epic notes line 8: "use week_17 folders for projected_points, then look at actual_points array in week_18 folders"
- I interpreted this as "week 17 is a special case"
- Should have realized: week_17 + week_18 example implies ALL weeks use week_N + week_N+1 pattern
- Wrote in spec.md: "NO special handling needed - arrays contain all data" (WRONG)

**Data Model Investigation Failure (Stage 2):**
- Never investigated json_exporter.py to understand how week folders are structured
- Never manually inspected data files to verify assumptions
- Assumed week_N folder contains week N actuals (WRONG)
- Reality: week_N folder has actuals for weeks 1 to N-1 only (point-in-time data)

**Implementation Without Validation (Stage 5b):**
- No hands-on data inspection before coding
- Never opened Python REPL to manually load and print data values
- Never verified that week_01 folder has 0.0 for week 1 actuals

**The "0.0 Acceptance" Catastrophe (Stage 5ca):**
- Smoke test output: "[PASS] 108/108 QB players have accessible week 1 data (0 have non-zero actual points)"
- **I marked this as PASSED** despite "(0 have non-zero points)" being statistically impossible
- No critical thinking: "Would this make sense in production?"
- No statistical sanity checks (zero percentage, variance, realistic ranges)

**Systemic Workflow Failures:**
- Spec treated as gospel (no "assume spec is wrong and validate" stage)
- No hands-on data inspection requirement
- Smoke testing checks structure, not semantics
- No statistical sanity validation
- Confirmation bias (looked for success, not failure)

----

PROPOSED SOLUTION:

**Code Changes Required:**

1. **Update AccuracySimulationManager.py (_load_season_data method):**
   - Currently returns: (week_N_folder, week_N_folder)
   - Should return: (week_N_folder, week_N+1_folder)
   - Handle week 17 specially: use week_18 for actuals

2. **Update AccuracySimulationManager.py (get_accuracy_for_week method):**
   - Load projections from week_N folder
   - Load actuals from week_N+1 folder
   - Merge data: use projected_points from week_N, actual_points from week_N+1

3. **Update ParallelAccuracyRunner.py (same pattern):**
   - Apply identical changes to parallel implementation
   - Ensure consistent behavior between serial and parallel modes

**Data Loading Pattern:**
```python
# For week 1 accuracy:
projected_path = "simulation/sim_data/{season}/weeks/week_01/"  # Projections
actual_path = "simulation/sim_data/{season}/weeks/week_02/"     # Actuals

# Use:
# - projected_points[0] from week_01 folder (what we projected for week 1)
# - actual_points[0] from week_02 folder (what actually happened in week 1)
```

**Special Case - Week 17:**
```python
# For week 17 accuracy:
projected_path = "simulation/sim_data/{season}/weeks/week_17/"  # Projections
actual_path = "simulation/sim_data/{season}/weeks/week_18/"     # Actuals

# Use:
# - projected_points[16] from week_17 folder
# - actual_points[16] from week_18 folder
```

----

VERIFICATION PLAN:

**Follow ALL Lessons Learned (MANDATORY):**

This bug fix MUST implement the 6 prevention strategies documented in:
- feature_02_accuracy_sim_json_integration/lessons_learned.md (lines 604-890)
- epic_lessons_learned.md (lines 184-223)

**Stage 2 (Spec Creation) - ENHANCED:**
1. Re-read epic notes with fresh eyes (ignore existing spec.md)
2. For EACH claim in spec, verify with actual code/data (not assumptions)
3. Manually inspect data files: week_01 vs week_02 actual_points values
4. Investigate json_exporter.py to understand data model
5. Document ALL assumptions with evidence (code reference OR data values)

**Stage 5a.5 (NEW - Hands-On Data Inspection) - MANDATORY:**
1. Open Python REPL
2. Load week_01/qb_data.json and week_02/qb_data.json
3. Print actual_points[0] from BOTH files
4. Verify week_01 has 0.0, week_02 has non-zero values
5. Document findings BEFORE implementing

**Stage 5ca (Smoke Testing) - ENHANCED:**
1. Part 3 E2E test MUST include statistical sanity checks:
   - Zero percentage: >90% zeros → AUTOMATIC FAIL
   - Variance: stddev = 0 → AUTOMATIC FAIL
   - Realistic ranges: MAE should be 3-8 for NFL data
2. Critical question: "If I saw these values in production, would I be suspicious?"
3. EXPLICIT RULE: Any output containing "0 have non-zero" → AUTOMATIC FAIL

**Stage 5cb (QC Rounds) - ENHANCED:**
1. Round 2: Add output validation iteration
   - Run actual MAE calculations
   - Verify realistic results (MAE in 3-8 range)
   - Check variance across positions
2. Round 3: Add data source validation
   - Critical question: "Are we loading from the RIGHT folder?"
   - Distinguish "access correctly" vs "correct data source"

**Verify ALL Epic Changes (USER REQUIREMENT):**

Before completing bug fix, MUST verify ALL changes made in this epic against:
1. Original epic notes (integrate_new_player_data_into_simulation_notes.txt)
2. Original simulation code (simulation/accuracy/AccuracySimulationManager.py - pre-epic)
3. Original data structures (how CSV vs JSON differ)
4. Cross-reference with Feature 01 (Win Rate Sim) to ensure consistency

**Verification Steps:**

How to verify fix works:
1. Load week_01 QB data → projected_points[0] should have values
2. Load week_02 QB data → actual_points[0] should have non-zero values
3. Run accuracy calculation for week 1
4. Verify MAE is realistic (3-8 range for QB)
5. Verify >0% of players have non-zero actual points
6. Run for ALL 17 weeks, verify week 17 uses week_18 folder
7. Compare results with historical accuracy data (if available)

**Statistical Sanity Checks:**
- Week 1 QB: At least 50% should have non-zero actual points
- MAE range: 3-8 points (NFL typical)
- Variance: stddev > 0 (not all identical)
- Zero percentage: <90% (most players score points)

----

USER NOTES:

This bug fix must:
1. Follow ALL 6 prevention strategies from lessons learned
2. Implement Stage 2.5 principles (assume spec is wrong, validate independently)
3. Implement Stage 5a.5 principles (hands-on data inspection before coding)
4. Use enhanced smoke testing (statistical sanity checks)
5. Use enhanced QC rounds (output validation, data source verification)
6. Verify ALL epic changes against original notes/code/documentation

This is a CATASTROPHIC bug that passed through 7 workflow stages. The fix must demonstrate that we've learned from this failure and will never accept impossible data (like "0 have non-zero points") again.
