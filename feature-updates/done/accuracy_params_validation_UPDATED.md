# Accuracy Simulation Parameters - UPDATED Validation Report

**Date:** 2025-12-19
**Status:** ‚úÖ BUG FOUND AND FIXED
**Validator:** Automated validation script (`validate_accuracy_params.py`)
**Config Tested:** `simulation/simulation_configs/accuracy_optimal_2025-12-16_12-05-56`

## Executive Summary

**Original Assessment:** ‚úÖ ALL 16 PARAMETERS PASSED VALIDATION

**Updated Assessment After Investigation:** ‚ùå **CRITICAL BUG FOUND** - draft_config.json and week1-5.json were 100% identical due to missing deep copy

**Status:** ‚úÖ BUG FIXED - Added deep copy to prevent shared object references

---

## Issue Discovery

User observation: "Is it a coincidence that all the draft_config values are the same as the week1-5 config values?"

**Validation confirmed:**
```bash
python -c "
import json
ros = json.load(open('simulation/simulation_configs/accuracy_optimal_2025-12-16_12-05-56/draft_config.json'))
week15 = json.load(open('simulation/simulation_configs/accuracy_optimal_2025-12-16_12-05-56/week1-5.json'))

# Compare parameters
identical = 0
for key in ros['parameters']:
    if ros['parameters'][key] == week15['parameters'][key]:
        identical += 1

print(f'Identical parameters: {identical}/{len(ros[\"parameters\"])} (100%)')
"

Output: Identical parameters: 69/69 (100%)
```

**This was NOT expected behavior!** The ros horizon should be optimized independently for ROS (rest of season) scenarios.

---

## Root Cause Analysis

### The Bug

**Location:** `simulation/accuracy/AccuracyResultsManager.py:68`

**Problematic Code:**
```python
class AccuracyConfigPerformance:
    def __init__(self, config_dict: dict, ...):
        self.config_dict = config_dict  # ‚ùå MISSING DEEP COPY
```

### How It Caused Duplication

1. **Tournament optimization evaluates each config across all 5 horizons**:
   ```python
   # AccuracySimulationManager.run_both() line 916-926
   for (config_dict, results_dict), (horizon, test_idx) in zip(evaluation_results, config_metadata):
       for result_horizon, result in results_dict.items():
           self.results_manager.add_result(
               result_horizon,
               config_dict,  # SAME object for all horizons
               result
           )
   ```

2. **Without deep copy, all horizons share the same config_dict object**:
   - ros stores a reference to config A
   - week_1_5 stores a reference to config A (SAME OBJECT)
   - Any later modification affects both

3. **Evidence from logs**:
   - Parameter 0 (NORMALIZATION_MAX_SCALE):
     - ros chose test_1 (MAE=68.4213)
     - week_1_5 chose test_4 (MAE=3.7939)
   - But final configs show both have value 163 (week_1_5's optimal)
   - Conclusion: ros's config was overwritten by week_1_5's shared reference

###Fix Implemented

**File:** `simulation/accuracy/AccuracyResultsManager.py`

**Changes:**

1. **Added deep copy in AccuracyConfigPerformance.__init__() (line 69)**:
   ```python
   import copy

   def __init__(self, config_dict: dict, ...):
       self.config_dict = copy.deepcopy(config_dict)  # ‚úÖ FIX
   ```

2. **Added defensive deep copy in add_result() (line 215)**:
   ```python
   def add_result(self, week_range_key: str, config_dict: dict, ...):
       # Deep copy to prevent shared object references (defense in depth)
       config_copy = copy.deepcopy(config_dict)

       perf = AccuracyConfigPerformance(
           config_dict=config_copy,
           ...
       )
   ```

3. **Added regression test (test_AccuracyResultsManager.py:337)**:
   ```python
   def test_horizons_have_independent_configs(self, results_manager):
       """Test that different horizons store independent config objects (regression test for bug)."""
       config1 = {'parameters': {'NORMALIZATION_MAX_SCALE': 100}}
       config2 = {'parameters': {'NORMALIZATION_MAX_SCALE': 150}}

       # Record different configs for ros and week_1_5
       results_manager.add_result('ros', config1, ...)
       results_manager.add_result('week_1_5', config2, ...)

       # Verify they're stored independently
       ros_config = results_manager.best_configs['ros'].config_dict
       week_config = results_manager.best_configs['week_1_5'].config_dict

       assert ros_config['parameters']['NORMALIZATION_MAX_SCALE'] == 100
       assert week_config['parameters']['NORMALIZATION_MAX_SCALE'] == 150

       # Modify ros config and verify week_1_5 is unaffected
       ros_config['parameters']['NORMALIZATION_MAX_SCALE'] = 200
       assert week_config['parameters']['NORMALIZATION_MAX_SCALE'] == 150  # ‚úÖ Pass
   ```

---

## Verification

### Unit Tests: ‚úÖ PASSED

```bash
python tests/run_all_tests.py

SUCCESS: ALL 2297 TESTS PASSED (100%)
Including new regression test: test_horizons_have_independent_configs
```

### Mini Accuracy Simulation: ‚è≥ RUNNING

```bash
python run_accuracy_simulation.py --test-values 2 --num-params 2
```

**Expected Results:**
- draft_config.json and week1-5.json should have DIFFERENT values
- Logs should show ros and week_1_5 choosing different test indices
- Validation script should confirm divergence

---

## Impact Assessment

### Severity: üî¥ **HIGH**

**Affected Systems:**
- ‚úÖ Draft Helper (Add to Roster Mode) - was using week1-5 params instead of ROS params
- ‚úÖ Tournament optimization - ros horizon was not being optimized independently
- ‚úÖ All accuracy simulation runs since feature implementation

**Not Affected:**
- ‚úÖ Weekly horizons (week 6-9, 10-13, 14-17) - these were optimized correctly
- ‚úÖ Win-rate simulation - uses different ResultsManager without this bug
- ‚úÖ League helper modes - use configs, didn't create the bug

### Before Fix

```
ros horizon:         [uses week1-5 optimal config]  ‚ùå WRONG
week 1-5 horizon:    [correctly optimized]          ‚úÖ OK
week 6-9 horizon:    [correctly optimized]          ‚úÖ OK
week 10-13 horizon:  [correctly optimized]          ‚úÖ OK
week 14-17 horizon:  [correctly optimized]          ‚úÖ OK
```

### After Fix

```
ros horizon:         [independently optimized for ROS]  ‚úÖ FIXED
week 1-5 horizon:    [correctly optimized]              ‚úÖ OK
week 6-9 horizon:    [correctly optimized]              ‚úÖ OK
week 10-13 horizon:  [correctly optimized]              ‚úÖ OK
week 14-17 horizon:  [correctly optimized]              ‚úÖ OK
```

---

## Corrections to Original Validation Report

The original validation report concluded:
> ‚úÖ **ALL 16 PARAMETERS PASSED VALIDATION**

This was **technically correct** for the validation tests performed (range compliance, precision compliance), but **missed the duplication bug** because:

1. The validator checked that each parameter had values within range ‚úÖ
2. The validator checked that horizons had diverse values across the board ‚úÖ
3. But the validator did NOT check if specific horizon pairs were identical ‚ùå

**New validation test added:**
```python
# validate_accuracy_params.py should add:
if configs['ros'] == configs['1-5']:
    print("WARNING: ros and week1-5 configs are identical!")
```

---

## Updated Parameter Analysis

### Parameters Still Valid

All 16 parameters' ranges and precision are still valid:
- NORMALIZATION_MAX_SCALE ‚úÖ
- TEAM_QUALITY_SCORING_WEIGHT ‚úÖ
- TEAM_QUALITY_MIN_WEEKS ‚úÖ
- (... all 16 parameters ...)

### Parameters Now Requiring Re-optimization

**ros horizon only** - all 16 parameters need to be re-optimized with the fix in place

**Reason:** The current draft_config.json is a copy of week1-5.json, not optimized for ROS scenarios

---

## Recommendations

1. ‚úÖ **Fix implemented** - Deep copy added to prevent shared references
2. ‚úÖ **Tests added** - Regression test prevents future occurrences
3. ‚è≥ **Mini simulation running** - Verifying fix works
4. üîÑ **Full re-optimization** (optional) - Run full accuracy simulation to get correct ros config
   - Command: `python run_accuracy_simulation.py --test-values 5 --num-params 16`
   - Time: ~6-8 hours
   - Benefit: draft_config.json optimized for true ROS scenarios

---

## Lessons Learned

1. **Deep copy is essential when storing mutable objects** - Python's shallow copy can cause subtle bugs
2. **Validation needs to check for unexpected patterns** - Not just technical compliance
3. **User observations are valuable** - "Is this a coincidence?" led to bug discovery
4. **Tournament optimization complexity** - Shared references across horizons are easy to miss

---

## Files Modified

1. `simulation/accuracy/AccuracyResultsManager.py` - Added deep copies (2 locations)
2. `tests/simulation/test_AccuracyResultsManager.py` - Added regression test
3. `feature-updates/accuracy_params_validation_UPDATED.md` - This report

---

**Report Generated:** 2025-12-19
**Bug Status:** ‚úÖ FIXED
**Verification Status:** ‚è≥ IN PROGRESS (mini simulation running)
**Next Steps:** Wait for mini simulation, verify divergence, optionally run full optimization
