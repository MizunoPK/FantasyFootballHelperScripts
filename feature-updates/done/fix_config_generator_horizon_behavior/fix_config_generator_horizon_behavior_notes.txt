# Fix ConfigGenerator Horizon Behavior

## Problem Statement

Both win-rate and accuracy simulations currently have incorrect behavior for multi-horizon parameter optimization. ConfigGenerator merges all horizon configs into a single baseline, then generates test configs from that merged baseline. This means all horizons start from the same parameter values, which defeats the purpose of having separate optimal configs per horizon.

**Current (Incorrect) Behavior:**
1. ConfigGenerator.load_baseline_from_folder() merges all 5 files into single unified config
   - Uses week1-5.json for week-specific params (arbitrary choice)
   - Results in ONE baseline config stored in config_generator.baseline_config
2. For each parameter, generates N test configs from this single merged baseline
3. Tests all N configs
4. AFTER testing, assigns different configs to different horizons based on performance
5. Problem: All test configs started from the same baseline, so we're not truly testing each horizon's optimal path

**Example of Current Behavior (Parameter 2):**
- After parameter 1, we have:
  - draft_config.json: NORMALIZATION_MAX_SCALE=150
  - week1-5.json: NORMALIZATION_MAX_SCALE=120
  - week6-9.json: NORMALIZATION_MAX_SCALE=180
  - week10-13.json: NORMALIZATION_MAX_SCALE=140
  - week14-17.json: NORMALIZATION_MAX_SCALE=160
- ConfigGenerator merges these, using week1-5.json → all test configs start with NORMALIZATION_MAX_SCALE=120
- For parameter 2 (TEAM_QUALITY_SCORING_WEIGHT), generates 20 test configs
- All 20 configs have NORMALIZATION_MAX_SCALE=120 (wrong!)
- Should generate configs starting from each horizon's best value (150, 120, 180, 140, 160)

## Desired (Correct) Behavior

**ConfigGenerator should maintain 5 separate horizon baselines and generate test values per horizon:**

**Key Design Decisions:**
1. **Deprecate NUM_PARAMETERS_TO_TEST** - ConfigGenerator will only generate test values for ONE parameter at a time
2. **5 baseline configs** - Store all 5 horizon files separately (no merging)
3. **5 test value arrays** - Generate independent test values for each horizon

**High-Level Flow:**

1. **Initialize from baseline folder:**
   - Load all 5 config files separately (draft_config.json, week1-5.json, week6-9.json, week10-13.json, week14-17.json)
   - Store as 5 separate baseline configuration dictionaries
   - Do NOT merge them into a single config

2. **Generate test values for a parameter:**
   - For each horizon, extract the baseline value for that parameter from its config file
   - Generate N random test values within the parameter's valid range
   - Store 5 arrays (one per horizon): `[baseline_value, random_1, random_2, ..., random_N]`
   - Each horizon gets DIFFERENT random values (independent exploration)

3. **Provide config to simulation:**
   - Simulation requests: "Give me config for horizon X, test value index Y, parameter Z"
   - ConfigGenerator takes the baseline config for horizon X
   - Replaces parameter Z's value with test_values[horizon_X][Y]
   - Returns complete config dictionary

**Example for NORMALIZATION_MAX_SCALE with 3 test values:**
- After parameter 1, we have 5 baseline configs with different NORMALIZATION_MAX_SCALE values
- ConfigGenerator generates 5 arrays (one per horizon):
  - ros: [150, 89, 167, 134] - starts from 150 (from draft_config.json)
  - week_1_5: [120, 91, 145, 158] - starts from 120 (from week1-5.json)
  - week_6_9: [180, 76, 199, 142] - starts from 180 (from week6-9.json)
  - week_10_13: [140, 103, 128, 166] - starts from 140 (from week10-13.json)
  - week_14_17: [160, 172, 144, 153] - starts from 160 (from week14-17.json)
- Total configs to test: 5 horizons × 4 values = 20 configs
- Each config starts from its horizon's optimal value from previous parameter

## Impact on Both Simulations

### Win-Rate Simulation
**Current total configs tested per parameter:**
- 20 test configs (all from same merged baseline)
- Each tested N times (simulations per config)
- Results assigned to different horizons after testing

**Correct total configs tested per parameter:**
- 5 horizons × 20 test values = 100 configs
- Each tested N times (simulations per config)
- Each horizon optimizes independently from its own best starting point

**Performance impact:**
- 5x more configs to test per parameter
- BUT this is the intended "tournament" behavior
- Each horizon's champion competes with new parameter value

### Accuracy Simulation (Both Mode)
**Current behavior:**
- Not yet implemented (this is why we discovered the issue)

**Correct behavior:**
- Same as win-rate: 5 horizons × 20 test values = 100 configs per parameter
- Each config evaluated across all 5 horizons for accuracy (MAE calculation)
- Best config for each horizon tracked independently

## File Structure

**Complete Config Folder Structure (6 files):**
- `league_config.json` - Base parameters shared by all horizons (non-week-specific)
- `draft_config.json` - Week-specific parameters for draft/season-long optimization (ROS horizon)
- `week1-5.json` - Week-specific parameters for weeks 1-5
- `week6-9.json` - Week-specific parameters for weeks 6-9
- `week10-13.json` - Week-specific parameters for weeks 10-13
- `week14-17.json` - Week-specific parameters for weeks 14-17

**Horizon Composition:**
Each horizon = league_config.json (base params) + its specific file (week-specific params)
- ROS horizon: league_config.json + draft_config.json
- 1-5 horizon: league_config.json + week1-5.json
- 6-9 horizon: league_config.json + week6-9.json
- 10-13 horizon: league_config.json + week10-13.json
- 14-17 horizon: league_config.json + week14-17.json

**draft_config.json Integration:**
- New file (not yet integrated into win-rate simulation)
- Same structure as week*.json files: {config_name, description, parameters}
- Contains week-specific params optimized for draft phase
- Win-rate simulation will use this when simulating drafts
- This feature integrates draft_config.json into the system

## Simulation Responsibilities

**Win-Rate Simulation:**
- Optimizes ONLY league_config.json parameters (base/shared params)
- Does NOT optimize horizon-specific parameters (draft_config.json, week*.json remain fixed)
- PARAMETER_ORDER contains only league_config.json params
- Still loads all 6 files, but only varies shared parameters during optimization
- Uses draft_config.json when simulating drafts (not for optimization)

**Accuracy Simulation:**
- Optimizes ALL parameters (both league_config.json AND horizon-specific params)
- PARAMETER_ORDER contains both base and week-specific params
- Tests configs across all 5 horizons (tournament optimization)
- This is where horizon-specific parameter optimization happens

**This Feature's Scope:**
- Refactor ConfigGenerator to support horizon-based approach (groundwork for both sims)
- Update win-rate sim to use new ConfigGenerator for league_config.json params
- Update accuracy sim to use new ConfigGenerator for full horizon optimization
- Integrate draft_config.json into both simulations

## Important Notes

**Deprecation:**
- Remove NUM_PARAMETERS_TO_TEST functionality from both simulations
- ConfigGenerator will only handle one parameter at a time
- Simpler implementation and clearer semantics

**Breaking Changes:**
- This is a breaking change for both simulations
- Old optimal configs are no longer valid (different algorithm, new file structure)
- Performance characteristics change (5x more configs tested per parameter in accuracy sim)
- Win-rate sim now uses 6-file structure (adds draft_config.json)
- May find different "optimal" parameters (more thorough exploration)
- Users should re-run simulations after this update

**Benefits:**
- Proper separation: win-rate optimizes shared params, accuracy optimizes horizon params
- Both simulations use consistent 6-file config structure
- Integrates draft_config.json for draft-phase optimization
- Accuracy sim implements true "tournament" optimization across horizons
- More computationally expensive (5x configs in accuracy) but finds better optima
- Matches original design intent documented in notes
