# Feature Request: Replace MAE with Ranking Accuracy Metrics in Accuracy Simulation

## Problem Statement

The current accuracy simulation optimizes for Mean Absolute Error (MAE), which measures exact point prediction accuracy. However, fantasy football decisions depend on relative rankings, not exact point totals.

**Current Issue:**
- Accuracy simulation finds optimal config by minimizing MAE
- Result: Disables most scoring features (TEAM_QUALITY=0.0, PERFORMANCE=0.01, MATCHUP=0.03)
- These "optimal" configs perform poorly for actual decisions
- Win-rate simulation with all features enabled achieves 77.8% win rate
- Accuracy-optimal configs would likely perform worse despite lower MAE

**Root Cause:**
MAE penalizes variance in predictions, even when that variance helps identify relative value. Adding matchup/performance adjustments might increase prediction error by 1-2 points per player, but correctly identifies which players will outperform/underperform their baseline.

## Proposed Solution

Replace MAE with ranking accuracy metrics that measure decision quality:

### Primary Metric: Pairwise Decision Accuracy
For every pair of players at same position:
- Does our scoring correctly predict which player will score more?
- Metric: % of pairwise comparisons where prediction matches actual result
- Example: If Player A scored > Player B scored 68% of times we predicted A > B, accuracy = 68%

### Secondary Metrics:
1. **Top-N Overlap Accuracy**
   - How many of our top 10 predicted players are actually top 10 scorers?
   - Test multiple N values: top 5, top 10, top 20
   - Critical for identifying elite players

2. **Spearman Rank Correlation**
   - Overall ranking order similarity (-1 to +1)
   - Standard statistical measure of ranking quality
   - Accounts for entire ranking distribution

3. **Positional Breakdown**
   - Calculate all metrics separately for QB, RB, WR, TE
   - Different positions may benefit from different adjustments
   - Identifies which scoring features help which positions

## Implementation Approach

### Phase 1: Add Ranking Metrics to AccuracyCalculator
Create new methods alongside existing calculate_mae():
- `calculate_pairwise_accuracy(projections, actuals, position)` → float (0.0-1.0)
- `calculate_top_n_accuracy(projections, actuals, n, position)` → float (0.0-1.0)
- `calculate_spearman_correlation(projections, actuals, position)` → float (-1.0 to +1.0)

### Phase 2: Integrate into AccuracySimulationManager
- Add ranking metrics to config evaluation
- Primary optimization target: Maximize pairwise accuracy
- Secondary targets: Top-10 accuracy, Spearman correlation
- Keep MAE as diagnostic metric (not optimization target)

### Phase 3: Update Results Tracking
- AccuracyResultsManager tracks multiple metrics per config
- Best config = highest pairwise accuracy (not lowest MAE)
- Results include all metrics for analysis:
  ```json
  {
    "pairwise_accuracy": 0.68,
    "top_10_accuracy": 0.75,
    "top_20_accuracy": 0.70,
    "spearman_correlation": 0.82,
    "mae": 3.45,  // diagnostic only
    "by_position": {
      "QB": {"pairwise": 0.71, "top_10": 0.80},
      "RB": {"pairwise": 0.66, "top_10": 0.70},
      "WR": {"pairwise": 0.65, "top_10": 0.65},
      "TE": {"pairwise": 0.72, "top_10": 0.85}
    }
  }
  ```

## Expected Outcomes

### Before (MAE Optimization):
```
TEAM_QUALITY_SCORING   WEIGHT: 0.0    ← Disabled
PERFORMANCE_SCORING    WEIGHT: 0.01   ← Disabled
MATCHUP_SCORING        WEIGHT: 0.03   ← Disabled
MAE: 3.33 points
Pairwise Accuracy: ~55-60% (estimated)
```

### After (Ranking Accuracy Optimization):
```
TEAM_QUALITY_SCORING   WEIGHT: 0.5-2.0  ← Enabled
PERFORMANCE_SCORING    WEIGHT: 2.0-4.0  ← Enabled
MATCHUP_SCORING        WEIGHT: 1.0-3.0  ← Enabled
MAE: 3.8-4.2 points (slightly higher, acceptable)
Pairwise Accuracy: 68-72% (much better decisions)
```

### Benefits:
- ✓ Accuracy simulation produces configs that actually help with decisions
- ✓ Aligns accuracy optimization with win-rate optimization
- ✓ Validates that scoring features provide value
- ✓ Can confidently use accuracy-optimal configs for starter decisions
- ✓ Better understanding of which features help which positions

## Success Criteria

1. **Accuracy-optimal configs enable scoring features** (weights > 0.5)
2. **High correlation with win-rate configs** (similar parameter values)
3. **Pairwise accuracy > 65%** for all positions
4. **Top-10 accuracy > 70%** for identifying elite players
5. **Production-ready configs** that users can trust for decisions

## Technical Considerations

### Data Requirements
- Same historical data (2021, 2022, 2024 seasons)
- Same week ranges (1-5, 6-9, 10-13, 14-17)
- Player projections and actual points per week

### Computational Impact
- Pairwise comparisons: O(n²) per position group
  - ~40 QBs = 1,600 pairs
  - ~80 RBs = 6,400 pairs
  - ~120 WRs = 14,400 pairs
  - ~40 TEs = 1,600 pairs
  - Total: ~24,000 comparisons per week
- Should be fast enough (current MAE is also O(n))

### Backward Compatibility
- Keep MAE calculation for comparison
- Old accuracy-optimal configs still valid (just not recommended)
- Can run both metrics side-by-side during transition

## Alternative Considered: Just Use Win-Rate Sim

**Option:** Deprecate accuracy simulation entirely, only use win-rate simulation.

**Rejected because:**
- Win-rate sim takes much longer (simulates full leagues)
- Accuracy sim is faster for rapid iteration
- Different use cases: win-rate = draft decisions, accuracy = weekly starters
- Ranking accuracy would make accuracy sim valuable again

## References

- Current accuracy simulation: `simulation/accuracy/AccuracySimulationManager.py`
- MAE calculation: `simulation/accuracy/AccuracyCalculator.py`
- Win-rate simulation: `simulation/win_rate/SimulationManager.py`
- Issue discovered: 2025-12-20 (accuracy sim disabling all scoring features)

## Priority: HIGH

This directly impacts the value and usability of the accuracy simulation system.

## Related Features

- May want to add similar metrics to win-rate simulation for analysis
- Could use ranking accuracy to validate starter helper mode recommendations
- Position-specific metrics could inform position-specific weights

---

## Next Steps

When ready to implement:
1. User: "Help me develop the ranking_accuracy_metrics feature"
2. Follow feature planning guide to create detailed specs
3. Design API for new ranking accuracy methods
4. Implement and test metrics
5. Update simulation to use new optimization target
6. Validate results match expectations
