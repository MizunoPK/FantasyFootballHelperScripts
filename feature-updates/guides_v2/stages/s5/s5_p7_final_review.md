# S5: Feature Implementation
## S5.P.3: Post-Implementation
### S5.P7: Final Review

**File:** `part_5.3.3_final_review.md`

**Purpose:** Production readiness validation through comprehensive code review, lessons learned capture, and final verification.

**Stage Flow Context:**
```
S5.P5 (Smoke Testing) â†’ S5.P6 (QC Rounds) â†’
â†’ [YOU ARE HERE: S5.P7 - Final Review] â†’
â†’ S5.P4 (Cross-Feature Alignment)
```

---

## ğŸš¨ MANDATORY READING PROTOCOL

**BEFORE starting Final Review, you MUST:**

1. **Use the phase transition prompt** from `prompts_reference_v2.md`
   - Find "Starting S5.P7 (Final Review)" prompt
   - Speak it out loud (acknowledge requirements)
   - List critical requirements from this guide

2. **Update README Agent Status** with:
   - Current Phase: POST_IMPLEMENTATION (Final Review)
   - Current Guide: stages/s5/final_review.md
   - Guide Last Read: {YYYY-MM-DD HH:MM}
   - Critical Rules: "PR review protocol MANDATORY", "Fresh agents for each round", "Update guides immediately", "100% completion required"
   - Next Action: READ pr_review_protocol.md and begin Round 1 specialized reviews

3. **Verify all prerequisites** (see checklist below)

4. **THEN AND ONLY THEN** begin final review

**This is NOT optional.** Reading this guide ensures production-ready quality.

---

## Quick Start

**What is this stage?**
Final Review is the production readiness validation where you perform 11-category PR review, immediately apply lessons learned to guides, verify 100% completion, and document feature completion.

**When do you use this guide?**
- S5.P6 complete (all 3 QC rounds passed)
- Ready for final production readiness validation
- Before cross-feature alignment

**Key Outputs:**
- âœ… PR review complete (11 categories checked, zero critical issues)
- âœ… lessons_learned.md updated
- âœ… Workflow guides updated immediately (lessons applied, not just documented)
- âœ… Final verification passed (100% completion confirmed)
- âœ… Ready for S5d (Cross-Feature Alignment)

**Time Estimate:**
30-45 minutes

**Exit Condition:**
Final Review is complete when PR review finds zero critical issues, lessons learned are applied to guides (not just documented), 100% completion is verified, and you're ready to proceed to Cross-Feature Alignment

---

## ğŸ›‘ Critical Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CRITICAL RULES - These MUST be copied to README Agent Status â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. âš ï¸ ALL 11 PR REVIEW CATEGORIES ARE MANDATORY
   - Cannot skip any category
   - Each category catches different issues
   - Must document findings for ALL categories (even if no issues)

2. âš ï¸ IF PR REVIEW FINDS CRITICAL ISSUES â†’ QC RESTART
   - Critical = correctness, security, breaking changes
   - Must follow QC Restart Protocol from S5.P6
   - Minor issues can be documented and don't block

3. âš ï¸ LESSONS LEARNED MUST UPDATE GUIDES
   - If you discover guide gaps â†’ update guides IMMEDIATELY
   - Don't just document the lesson â†’ apply it to guides
   - Update relevant guide files before completing S5.P7
   - This is NOT optional

4. âš ï¸ 100% REQUIREMENT COMPLETION - ZERO TECH DEBT TOLERANCE
   - Feature is DONE or NOT DONE (no partial credit, no "90% done")
   - ALL spec requirements must be implemented 100%
   - ALL checklist items must be verified and resolved
   - NO "we'll add that later" items allowed
   - NO deferred features, shortcuts, or "temporary" solutions
   - NO tech debt - if it's in the spec, it's REQUIRED and must be fully implemented
   - If something cannot be implemented, get user approval to REMOVE from scope
   - Clean codebase with zero compromises - every requirement fully complete

5. âš ï¸ FINAL VERIFICATION IS MANDATORY
   - Cannot skip final verification checklist
   - Must honestly answer: "Would I ship this to production?"
   - If any hesitation â†’ investigate why

6. âš ï¸ RE-READING CHECKPOINT
   - Before declaring complete â†’ re-read Completion Criteria
   - Verify ALL criteria met (not just most)
   - Update README Agent Status one final time
```

---

## Prerequisites Checklist

**Verify these BEFORE starting Final Review:**

**From S5.P6 (QC Rounds):**
- [ ] QC Round 1: PASSED (<3 critical, >80% requirements)
- [ ] QC Round 2: PASSED (all Round 1 issues resolved, zero new critical)
- [ ] QC Round 3: PASSED (ZERO issues found)
- [ ] All re-reading checkpoints completed

**From S5.P5 (Smoke Testing):**
- [ ] All 3 smoke test parts passed
- [ ] Part 3 verified OUTPUT DATA VALUES

**Unit Tests:**
- [ ] Run `python tests/run_all_tests.py` â†’ exit code 0
- [ ] All unit tests passing (100% pass rate)

**Documentation:**
- [ ] `code_changes.md` fully updated
- [ ] `implementation_checklist.md` all requirements verified
- [ ] QC round results documented

**If ANY prerequisite not met:** Return to previous stage and complete it first.

---

## Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                FINAL REVIEW WORKFLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PR Review Checklist (11 Categories)
   â”œâ”€ 1. Correctness and Logic
   â”œâ”€ 2. Code Quality and Readability
   â”œâ”€ 3. Comments and Documentation
   â”œâ”€ 4. Refactoring Concerns
   â”œâ”€ 5. Testing
   â”œâ”€ 6. Security
   â”œâ”€ 7. Performance
   â”œâ”€ 8. Error Handling
   â”œâ”€ 9. Architecture and Design
   â”œâ”€ 10. Compatibility and Integration
   â”œâ”€ 11. Scope and Focus
   â†“
   Evaluate: Critical issues? â†’ If YES: QC Restart
             Minor issues only? â†’ Document and proceed

Lessons Learned Capture
   â”œâ”€ Review what went well / what didn't
   â”œâ”€ Identify guide gaps
   â”œâ”€ UPDATE GUIDES IMMEDIATELY (don't just document)
   â”œâ”€ Update lessons_learned.md
   â†“

Final Verification
   â”œâ”€ All completion criteria met?
   â”œâ”€ Feature is ACTUALLY complete?
   â”œâ”€ Would ship to production?
   â†“
   If YES: Update README, proceed to S5d
   If NO: Investigate and resolve

Re-Reading Checkpoint
   â†“ Re-read Completion Criteria
   â†“ Update README Agent Status
   â†“ Ready for S5d
```

---

## Step 1: PR Review (Multi-Round with Fresh Eyes)

**ğŸš¨ MANDATORY: READ PR REVIEW PROTOCOL**

**Before proceeding, you MUST:**
1. **READ:** `stages/s5/s5_pr_review_protocol.md`
2. **Follow the complete hybrid approach:**
   - Round 1: 4 specialized reviews (fresh agent for each)
   - Rounds 2-5: Iterative comprehensive reviews (fresh agent for each)
   - 2 consecutive clean rounds required to pass
   - Maximum 5 rounds total

**Purpose:** Systematic PR review using fresh agent context to catch issues before committing.

**Why fresh agents?** New agents avoid context bias and provide "fresh eyes" on code changes.

---

### PR Review Protocol Summary

**You MUST follow pr_review_protocol.md completely. Key points:**

**Round 1: Specialized Reviews** (spawn 4 fresh agents via Task tool)
- Round 1a: Code Quality Review
- Round 1b: Test Coverage Review
- Round 1c: Security Review
- Round 1d: Documentation Review

**Consolidate Round 1 results:**
- If issues found â†’ Fix all â†’ Restart from Round 1a
- If multi-approach issues â†’ Escalate to user
- If NO issues â†’ Proceed to Round 2

**Rounds 2-5: Iterative Comprehensive Reviews** (spawn 1 fresh agent per round)
- Full checklist: Code Quality, Testing, Security, Documentation, Spec Alignment, Implementation Plan Alignment, Tech Debt, Performance
- Continue until 2 consecutive clean rounds
- If issues found â†’ Fix all â†’ Restart next round
- If 5 rounds without 2 consecutive clean â†’ Escalate to user

**Completion:**
- 2 consecutive clean rounds = PASSED âœ…
- Create `pr_review_issues.md` tracking all findings

---

### Quick Reference: 11-Category Checklist (used in Rounds 2-5)

**For detailed examples and guidance, see pr_review_protocol.md**

### Category 1: Correctness and Logic

- [ ] Does the code accomplish what it claims to do?
- [ ] Any logic errors? (off-by-one, incorrect conditionals, wrong operators)
- [ ] Edge cases and boundary conditions handled?
- [ ] Null/undefined handling appropriate?
- [ ] Calculations are mathematically correct?
- [ ] Loops terminate correctly?

**Common issues:**
- Off-by-one errors in loops (`range(n)` vs `range(n+1)`)
- Wrong comparison operators (`<` vs `<=`)
- Integer division when float needed (`5/2 = 2` in Python 2)

**Example:**
```python
# âŒ Off-by-one error
for i in range(len(players)):  # Correct
    player = players[i]

for i in range(len(players) + 1):  # âŒ Will crash on last iteration
    player = players[i]

# âœ… Correct comparison
if player.adp_rank < 50:  # Top 50 players
    apply_bonus()

if player.adp_rank <= 50:  # Depends on requirement (inclusive vs exclusive)
```

**Document findings:**
```markdown
### Category 1: Correctness and Logic
âœ… No issues found
- All loops verified for correct range
- All comparisons checked against spec
- Edge cases tested (empty list, single item, max size)
```

---

### Category 2: Code Quality and Readability

- [ ] Code is easy to understand without excessive mental overhead?
- [ ] Variable/function/class names are descriptive and consistent?
- [ ] Functions are appropriately sized (not doing too much)?
- [ ] Unnecessary complexity that could be simplified?
- [ ] Code follows project conventions (see CLAUDE.md)?
- [ ] No "clever" code that's hard to understand?

**Example - Bad vs Good:**
```python
# âŒ BAD - Unclear, does too much
def proc(d):
    r = []
    for x in d:
        if x['s'] > 10:
            r.append({'n': x['n'], 'v': x['s'] * 1.5})
    return sorted(r, key=lambda y: y['v'], reverse=True)

# âœ… GOOD - Clear, focused functions
def filter_high_scorers(players, threshold=10):
    """Return players with score above threshold."""
    return [p for p in players if p['score'] > threshold]

def apply_multiplier(players, multiplier=1.5):
    """Apply multiplier to player scores."""
    return [{'name': p['name'], 'value': p['score'] * multiplier}
            for p in players]

def sort_by_value(players, descending=True):
    """Sort players by value."""
    return sorted(players, key=lambda p: p['value'], reverse=descending)
```

---

### Category 3: Comments and Documentation

- [ ] Comments explain "why" rather than restating "what"?
- [ ] Public APIs adequately documented (docstrings)?
- [ ] Complex logic has explanatory comments?
- [ ] No stale or misleading comments?
- [ ] Type hints present (per CLAUDE.md standards)?
- [ ] **Code quality issues fixed immediately (NOT deferred)?**
  - Check: No "TODO" comments for code quality issues
  - Check: No "will fix later" notes
  - Check: All type hints present and complete (not deferred)
  - Check: All docstrings complete (not marked as "add later")
  - **If ANY issues found: Fix NOW before proceeding**
  - **Remember: "Later" often never comes - zero tech debt tolerance**

**Example:**
```python
# âŒ BAD - Restates code
# Loop through players
for player in players:
    # Add to list
    results.append(player)

# âœ… GOOD - Explains why
# Filter to only rostered players for trade analysis
# (Free agents handled separately in draft mode)
for player in players:
    if player.is_rostered:
        results.append(player)
```

---

### Category 4: Refactoring Concerns

- [ ] Does change introduce duplication that should be abstracted?
- [ ] Opportunities to improve existing code touched by this change?
- [ ] Change consistent with existing patterns in codebase?
- [ ] Could similar logic be unified?

**Example:**
```python
# âŒ DUPLICATION - Same logic in 3 places
# In DraftHelper:
if player.injury_status == "Out":
    penalty = -10
elif player.injury_status == "Questionable":
    penalty = -5

# In TradeSimulator:
if player.injury_status == "Out":
    penalty = -10
elif player.injury_status == "Questionable":
    penalty = -5

# âœ… REFACTORED - Unified in ConfigManager
# In ConfigManager:
def get_injury_penalty(self, injury_status):
    return self.config['injury_penalties'].get(injury_status, 0)

# In DraftHelper & TradeSimulator:
penalty = config.get_injury_penalty(player.injury_status)
```

---

### Category 5: Testing

- [ ] Sufficient unit/integration tests for new functionality?
- [ ] Tests cover edge cases and failure modes?
- [ ] Existing tests still valid, or need updates?
- [ ] Tests are meaningful (not just coverage theater)?
- [ ] Mock usage is appropriate (not excessive)?

**Red flags:**
- New feature with zero tests
- Tests that always pass (testing mocks, not real code)
- Tests with no assertions
- Tests that don't actually test the feature

---

### Category 6: Security

- [ ] Input validation and sanitization present?
- [ ] Authentication/authorization checks (if applicable)?
- [ ] No sensitive data exposure (logs, errors, responses)?
- [ ] No injection vulnerabilities (SQL, XSS, command injection)?
- [ ] File path handling safe (no path traversal)?
- [ ] API keys/secrets not hardcoded?

**Common issues:**
- User input used in file paths without validation
- Sensitive data (passwords, API keys) logged
- SQL queries built with string concatenation (SQL injection)

---

### Category 7: Performance

- [ ] No inefficient algorithms or data structures?
- [ ] No unnecessary loops or redundant calculations?
- [ ] Large data handled efficiently (not loading everything in memory)?
- [ ] No N+1 query patterns?
- [ ] Caching used appropriately?

**Example - Performance Issue:**
```python
# âŒ BAD - O(nÂ²) when O(n) possible
for player in all_players:
    for team_player in team_roster:  # Inner loop runs for EACH player
        if player.name == team_player.name:
            player.is_rostered = True

# âœ… GOOD - O(n) with set lookup
rostered_names = {p.name for p in team_roster}
for player in all_players:
    player.is_rostered = player.name in rostered_names  # O(1) lookup
```

---

### Category 8: Error Handling

- [ ] Errors caught and handled appropriately?
- [ ] Error messages helpful for debugging?
- [ ] Logging sufficient but not excessive?
- [ ] No bare `except:` clauses (too broad)?
- [ ] Resources cleaned up in error cases (files, connections)?
- [ ] Errors don't expose sensitive info?

**Example:**
```python
# âŒ BAD - Swallows all errors, no info
try:
    load_data()
except:
    pass

# âœ… GOOD - Specific exception, helpful error
try:
    load_data()
except FileNotFoundError as e:
    logger.error(f"Failed to load player data: {e}")
    raise DataProcessingError("Player data file not found", context=ctx)
```

---

### Category 9: Architecture and Design

- [ ] Change fits overall system architecture?
- [ ] Dependencies flow in right direction (no circular)?
- [ ] Appropriate separation of concerns?
- [ ] Not creating architectural debt?
- [ ] Follows existing patterns in codebase?

**Red flags:**
- Business logic in UI layer
- Tight coupling between unrelated modules
- Circular dependencies
- God objects (classes doing too much)

---

### Category 10: Compatibility and Integration

- [ ] Backwards compatibility maintained (if required)?
- [ ] No breaking changes to existing APIs?
- [ ] Configuration changes handled gracefully?
- [ ] Dependencies appropriate and justified?
- [ ] Works with existing features (not just in isolation)?

**Example:**
```python
# âŒ BREAKING CHANGE - Changed method signature
# Before:
def calculate_score(player):
    ...

# After (BREAKS all existing callers):
def calculate_score(player, config):
    ...

# âœ… BACKWARDS COMPATIBLE - Added optional parameter
def calculate_score(player, config=None):
    if config is None:
        config = ConfigManager()
    ...
```

---

### Category 11: Scope and Focus

- [ ] Change addresses stated requirements (not scope creep)?
- [ ] No unnecessary "improvements" beyond spec?
- [ ] Not over-engineered for current needs?
- [ ] Each change has clear justification?

**Example:**
```markdown
Spec requirement: "Add ADP multiplier to draft recommendations"

âœ… In scope:
- Calculate ADP multiplier
- Apply to draft scores
- Display in recommendations

âŒ Out of scope (unless explicitly discussed):
- Redesign entire scoring algorithm
- Add caching layer for performance
- Create configuration UI for ADP weights
- Implement machine learning model for ADP prediction
```

---

### PR Review Execution

**ğŸš¨ CRITICAL: Do NOT execute PR review manually from this guide**

**Instead:**

1. **READ:** `stages/s5/s5_pr_review_protocol.md` (complete protocol)

2. **Follow hybrid approach:**
   - Spawn fresh agents via Task tool for each review round
   - Track all findings in `pr_review_issues.md`
   - Continue until 2 consecutive clean rounds

3. **After PR review PASSED:**
   - Verify pr_review_issues.md shows final status: PASSED
   - Verify 2 consecutive clean rounds achieved
   - Proceed to Step 2 (Lessons Learned)

**The 11 categories above are REFERENCE ONLY** - they are automatically used by fresh agents during Rounds 2-5. You do NOT manually execute them from this guide.

**See pr_review_protocol.md for complete execution instructions.**

---

## Step 2: Lessons Learned Capture

**Purpose:** Document what went well, what didn't, and UPDATE GUIDES IMMEDIATELY

**CRITICAL:** Don't just document lessons - APPLY them to guides before completing S5.P7

---

### Lessons Learned Process

**1. Review what went well:**
- What aspects of implementation went smoothly?
- What parts of the guides were helpful?
- What practices prevented issues?

**2. Review what didn't go well:**
- What issues were discovered in QC/smoke testing?
- What was unclear in the guides?
- What steps were skipped/missed?
- What caused rework?

**3. Identify guide gaps:**
- Are there missing steps in guides that would have helped?
- Are there unclear instructions that caused confusion?
- Are there missing examples that would clarify?
- Are there missing anti-patterns to document?

**4. UPDATE GUIDES IMMEDIATELY:**

This is NOT optional. If you found guide gaps, fix them NOW.

**Example:**
```markdown
## Lesson Learned:

Issue: QC Round 2 found all output data was zeros
Root cause: Smoke test Part 3 only checked "file exists", didn't verify data VALUES
Guide gap: stages/s5/smoke_testing.md didn't emphasize DATA VALUES enough

Action taken: Updated S5.P5 guide
- Added "CRITICAL - Verify OUTPUT DATA" to Part 3 heading
- Added real-world example of zero data issue
- Added explicit "Don't just check file exists" warning
- Added code example showing good vs bad Part 3 validation

Files updated:
- feature-updates/guides_v2/stages/s5/smoke_testing.md
```

**5. Update lessons_learned.md:**

```markdown
## Feature_XX Lessons Learned

### What Went Well
- Smoke testing caught integration bug that unit tests missed
- QC Round 2 baseline comparison revealed pattern inconsistency
- PR Review Category 5 (Testing) identified missing edge case tests

### What Didn't Go Well
- Initial smoke test Part 3 only checked file existence (not data values)
- Required QC restart after Round 1 due to mock assumption failures
- 3 hours spent debugging issue that better interface verification would have caught

### Root Causes
- Guide didn't emphasize DATA VALUES enough in smoke testing
- Skipped Interface Verification Protocol in S5b (assumed interface)
- Excessive mocking in tests hid real integration issues

### Guide Updates Applied
1. Updated stages/s5/smoke_testing.md:
   - Enhanced smoke test Part 3 with DATA VALUES emphasis
   - Added real-world example of zero data issue

2. Updated stages/s5/implementation_execution.md:
   - Made Interface Verification Protocol STEP 1 (not optional)
   - Added "NO coding from memory" critical rule

3. Updated stages/s5/round1_todo_creation.md:
   - Enhanced Mock Audit (iteration 21) with "excessive mocking" anti-pattern

### Recommendations for Future Features
- ALWAYS verify data VALUES in smoke tests (not just structure)
- NEVER skip Interface Verification Protocol
- Use mocks only for I/O, not internal classes
- If in doubt about interface, READ THE SOURCE CODE

### Time Impact
- Guide gaps cost: ~3 hours debugging + 2 hours rework
- Following guides correctly would have saved: ~5 hours
- QC restart added: ~2 hours (but prevented larger issues later)
```

---

## Step 3: Final Verification

**Purpose:** Confirm all completion criteria met before transitioning to S5d

---

### Final Verification Checklist

**Smoke Testing:**
- [ ] Part 1 (Import Test): PASSED
- [ ] Part 2 (Entry Point Test): PASSED
- [ ] Part 3 (E2E Execution Test): PASSED with data VALUES verified

**QC Rounds:**
- [ ] QC Round 1 (Basic Validation): PASSED
- [ ] QC Round 2 (Deep Verification): PASSED
- [ ] QC Round 3 (Final Skeptical Review): PASSED (zero issues)

**PR Review:**
- [ ] All 11 categories reviewed
- [ ] Zero critical issues
- [ ] Minor issues documented (if any)

**Artifacts Updated:**
- [ ] lessons_learned.md updated with this feature's lessons
- [ ] Guides updated if gaps found (applied immediately, not just documented)
- [ ] Epic Checklist updated: `- [x] Feature_XX QC complete`

**Zero Tech Debt Verification:**
- [ ] **ZERO tech debt**: No deferred issues of ANY size (critical, minor, cosmetic)
- [ ] **ZERO "later" items**: If you wrote it down to fix later, fix it NOW
- [ ] **Production ready**: Would you ship this to production RIGHT NOW with no changes? (Must answer YES)

**README Agent Status:**
- [ ] Updated with completion of S5.P7
- [ ] Next action set to "S5d: Cross-Feature Alignment"

**Git:**
- [ ] All implementation changes committed
- [ ] Working directory clean (`git status`)
- [ ] Commit messages descriptive

**Final Question:**
- [ ] **"Is this feature ACTUALLY complete and ready for production?"**
  - Not "tests pass"
  - Not "code works"
  - But "feature is DONE and CORRECT"

**If ALL boxes checked:** Proceed to S5d
**If ANY box unchecked:** Do NOT proceed - complete the missing item first

---

## ğŸ”„ Re-Reading Checkpoint

**STOP - Before declaring S5.P7 complete:**

1. **Re-read "Completion Criteria" section below**
2. **Verify ALL criteria met (not just most)**
3. **Re-read "Prerequisites for Next Stage"**
4. **Update README Agent Status:**
   ```markdown
   Guide Last Re-Read: {timestamp}
   Checkpoint: S5.P7 complete, ready for S5d
   Current Phase: Cross-Feature Alignment (S5d)
   Next Action: Read stages/s5/post_feature_alignment.md
   ```

---

## Completion Criteria

**S5.P7 (and entire S5c) is complete when ALL of the following are true:**

### Smoke Testing (S5.P5)
- [x] All 3 smoke test parts passed
- [x] Part 3 verified OUTPUT DATA VALUES (not just "file exists")
- [x] Feature executes end-to-end without crashes
- [x] Output data is correct and reasonable

### QC Rounds (S5.P6)
- [x] QC Round 1 passed (<3 critical issues, >80% requirements met)
- [x] QC Round 2 passed (all Round 1 issues resolved, zero new critical issues)
- [x] QC Round 3 passed (ZERO issues found in skeptical review)

### PR Review (S5.P7)
- [x] All 11 categories reviewed
- [x] Zero critical issues found
- [x] Minor issues documented (if any exist)

### Documentation
- [x] lessons_learned.md updated with this feature's lessons
- [x] Guides updated if gaps were found (applied immediately)
- [x] code_changes.md complete and accurate
- [x] implementation_checklist.md all requirements verified

### Unit Tests
- [x] Run `python tests/run_all_tests.py` â†’ exit code 0
- [x] 100% pass rate maintained

### Git
- [x] All implementation changes committed
- [x] Working directory clean (`git status`)
- [x] Commit messages descriptive

### README Agent Status
- [x] Updated to reflect S5.P7 completion
- [x] Next action set to "S5d: Cross-Feature Alignment"
- [x] Guide Last Read timestamp current

### Final Verification
- [x] Feature is ACTUALLY complete (not just functional)
- [x] Would ship to production with confidence
- [x] Data values verified (not just structure)
- [x] All spec requirements met (100%, no partial work)

**If ALL criteria met:** Proceed to S5d (Cross-Feature Alignment)

**If ANY criteria not met:** Do NOT proceed until all are met

---

## Common Mistakes to Avoid

### Anti-Pattern 1: Documenting Lessons But Not Applying Them

**âŒ Mistake:**
```markdown
## Lessons Learned
- S5.P5 guide should emphasize data values more

{End of feature work - guide never updated}
```

**Why wrong:** Next feature will hit same issue because guide wasn't fixed

**âœ… Correct:** UPDATE GUIDES IMMEDIATELY when gaps found (Step 2)

---

### Anti-Pattern 2: Ignoring Minor Issues

**âŒ Mistake:**
"PR Review found missing docstrings, but that's minor, I'll skip documenting it"

**Why wrong:**
- Minor issues accumulate â†’ technical debt
- Missing docstrings â†’ harder maintenance
- Not documenting â†’ issue gets forgotten

**âœ… Correct:** Document ALL issues (critical AND minor), even if not blocking

---

### Anti-Pattern 3: "Good Enough" Mentality

**âŒ Mistake:**
"Feature mostly works, 90% of data is correct, ship it"

**Why wrong:**
- 10% wrong data â†’ untrustworthy results â†’ users abandon feature
- "Good enough" compounds â†’ technical debt
- Critical Rule: NO PARTIAL WORK

**âœ… Correct:** 100% requirement completion, or feature is INCOMPLETE

---

### Anti-Pattern 4: Skipping Final Verification

**âŒ Mistake:**
"I did PR review and lessons learned, that's enough"

**Why wrong:** Final Verification catches edge cases missed in earlier steps

**âœ… Correct:** Actually work through Final Verification Checklist (all boxes)

---

## Real-World Examples

### Example 1: Lessons Learned Updates Prevent Future Issues

**Feature:** Schedule strength multiplier

**Issue found in QC Round 2:**
```
Log quality check: 487 WARNING messages during normal execution
Investigation: Most are "Opponent data missing for week {week}, using default"

Root cause: Schedule data only loaded for weeks 1-17
Code tries to access week 18 (doesn't exist in regular season)
Should use INFO level, not WARNING (this is expected behavior)
```

**Developer's actions:**
1. Fixed log level (WARNING â†’ INFO)
2. Added documentation about 17-week schedule
3. **Updated S5.P6 guide:**
   - Added "Log Quality Verification" example
   - Added "Expected vs Unexpected warnings" distinction
   - Added this real-world example to guide

**Result:** Next feature developer read updated guide, avoided same issue

**Lesson:** Updating guides immediately prevents future features from hitting same issues.

---

### Example 2: PR Review Catches Scope Creep

**Feature:** Add ADP multiplier to draft recommendations

**PR Review Category 11 (Scope):**
```markdown
Spec requirement: "Add ADP multiplier to draft recommendations"

Code review found:
âœ… In scope:
- Calculate ADP multiplier
- Apply to draft scores
- Display in recommendations

âš ï¸ Out of scope (not in spec):
- NEW: Caching layer for ADP data (250 lines of code)
- NEW: Admin UI for configuring ADP weights (180 lines of code)
- NEW: ADP trend analysis over time (320 lines of code)

Total: 750 lines of unspecified code (30% of feature)

Issue: Scope creep - added features not in spec
Decision: Remove out-of-scope code or get user approval
```

**Resolution:**
- Asked user if these additions were wanted
- User said: "No, just the basic ADP multiplier for now"
- Removed 750 lines of unspecified code
- Feature size reduced 30%
- Complexity reduced significantly

**Lesson:** PR Review Category 11 catches scope creep before it's shipped

---

## Prerequisites for Next Stage

**Before transitioning to S5d (Cross-Feature Alignment), verify:**

### Completion Verification
- [ ] All S5.P7 completion criteria met (see Completion Criteria section)
- [ ] All smoke tests passed (3 parts)
- [ ] All QC rounds passed (3 rounds)
- [ ] PR review complete (11 categories)
- [ ] Lessons learned captured AND guides updated

### Files Verified
- [ ] lessons_learned.md updated
- [ ] code_changes.md complete
- [ ] implementation_checklist.md all verified
- [ ] Guides updated if gaps found

### Git Status
- [ ] All changes committed
- [ ] Working directory clean
- [ ] Descriptive commit messages

### README Agent Status
- [ ] Updated to reflect S5.P7 completion
- [ ] Next action set to "Read stages/s5/post_feature_alignment.md"

### Final Check
- [ ] Feature is COMPLETE (not just functional)
- [ ] Would ship to production with confidence
- [ ] 100% requirement completion (no partial work)

**If ALL verified:** Ready for S5d

**S5d Preview:**
- Review all REMAINING (not-yet-implemented) feature specs
- Compare to ACTUAL implementation (not plan) of just-completed feature
- Update specs if implementation revealed changes/insights
- Ensure remaining features align with reality

**Next step:** Read stages/s5/post_feature_alignment.md and use phase transition prompt

---

## Summary

**S5.P7 validates production readiness through:**
1. **PR Review** - 11 categories ensure code quality, security, correctness
2. **Lessons Learned** - Capture insights and apply improvements to guides
3. **Final Verification** - Confirm 100% completion and readiness

**Critical protocols:**
- All 11 PR categories mandatory (each catches different issues)
- Update guides immediately when gaps found (don't just document)
- 100% completion required (no partial work)
- Final verification confirms "actually complete" not just "functional"

**Success criteria:**
- PR review complete (zero critical issues)
- Lessons learned captured and guides updated
- Final verification passed (all boxes checked)
- Feature is COMPLETE and production-ready

**After S5.P7:** Proceed to S5d (Cross-Feature Alignment) to ensure remaining feature specs align with actual implementation.

---

*End of stages/s5/final_review.md*
