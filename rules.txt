## OBJECTIVE PLANNING WORKFLOW

Before starting changes, follow this mandatory workflow:

### **STEP 1: Create Draft TODO File**

Create an initial TODO file that maps out all the tasks needed to accomplish the objective. This draft is based solely on the original specification file (`updates/{objective_name}.txt`). The TODO file should be named `updates/todo-files/{objective_name}_todo.md`.

The draft TODO should include:
- High-level phases and tasks
- Anticipated file modifications
- Testing requirements
- Documentation updates

As you complete tasks, keep the file updated with your progress in case a new Claude agent in a new session needs to finish the work. Ensure this TODO file has everything it needs to maintain consistent work - including a note about keeping the file up to date on progress made.

### **STEP 2: First Verification Round (7 Iterations)**

Execute the TODO FILE VERIFICATION AND REFINEMENT PROTOCOL (detailed below) with **7 complete iterations** to research the codebase, identify patterns, and refine the draft TODO file. This happens BEFORE creating the questions file.

**Iteration Breakdown**:
- **Iterations 1-3**: Standard verification (research, cross-reference, refine)
- **Iteration 4**: Continue refinement with deeper technical details
- **Iteration 5**: **END-TO-END DATA FLOW VERIFICATION** - For each requirement, trace from entry point to output. Identify what calls each new method. Document integration points.
- **Iteration 6**: **SKEPTICAL RE-VERIFICATION** - Assume nothing written so far is accurate. Re-verify ALL claims, assumptions, file paths, method names, patterns, and implementation strategies from scratch. Question everything and validate with fresh codebase research.
- **Iteration 7**: **INTEGRATION GAP CHECK** - Review all new methods/classes planned. For each one, verify the TODO includes a task to modify the CALLER. If a new method has no caller modification task, add one.

### **STEP 3: Create Questions File**

After completing the first 7 verification iterations (including skeptical re-verification and integration gap check), create a questions file based on ambiguities, implementation choices, and user preference decisions discovered during codebase research. The questions file should be named `updates/{objective_name}_questions.md`.

This file should contain:
- Clarifying questions about requirements
- Implementation approach options (with recommendations based on codebase research)
- User preference questions
- Questions that arose during the first verification round

**IMPORTANT**: Wait for the user to answer these questions before proceeding to Step 4.

### **STEP 4: Update TODO with Question Answers**

After receiving user answers, update the TODO file to reflect:
- User's chosen implementation approaches
- Clarified requirements from answers
- Adjusted task priorities based on answers
- Any new tasks revealed by the answers

### **STEP 5: Second Verification Round (9 More Iterations)**

Execute the TODO FILE VERIFICATION AND REFINEMENT PROTOCOL again with **9 additional complete iterations** to:
- Validate that question answers are fully integrated
- Research any new patterns needed based on answers
- Refine implementation details
- Finalize task order and dependencies
- Verify end-to-end integration

**Iteration Breakdown**:
- **Iterations 8-10**: Verification with user answers integrated
- **Iteration 11**: Continue refinement with implementation-specific details
- **Iteration 12**: **END-TO-END DATA FLOW VERIFICATION** - Re-trace from entry point to output for each requirement with user answers integrated. Update integration points based on chosen approaches.
- **Iteration 13**: **SKEPTICAL RE-VERIFICATION** - Again, assume nothing is accurate. Re-verify ALL claims, especially those based on user answers. Validate that answers are correctly interpreted and integrated. Fresh codebase research required.
- **Iteration 14**: **INTEGRATION GAP CHECK** - Final review of all new methods/classes. For EACH new component, document: (1) what file it's in, (2) what existing code will call it, (3) what modification is needed in the caller. Flag any orphan code.
- **Iterations 15-16**: Final refinement, preparation for implementation, and creation of integration verification checklist

After completing both verification rounds (**16 total iterations**), the TODO file should be comprehensive, thoroughly validated, and ready for implementation.

---

## TODO FILE VERIFICATION AND REFINEMENT PROTOCOL

**üö® MANDATORY** - This iterative verification protocol must be executed:
1. After creating the draft TODO file (7 iterations - including data flow verification, skeptical re-verification, and integration gap check)
2. After receiving user's answers to questions (9 more iterations - including data flow verification, skeptical re-verification, and integration gap check)

### **WHEN TO EXECUTE**:
- ‚úÖ Immediately after generating the draft TODO file (STEP 2 - First 7 iterations)
- ‚úÖ After updating TODO with user's question answers (STEP 5 - Second 9 iterations)
- ‚úÖ Before beginning any implementation work

### **ITERATIVE REFINEMENT PROCESS (16 TOTAL ITERATIONS)**:

**üìã ITERATION 1: Initial Verification**

1. **Re-read ALL Source Documents**:
   - Open and carefully read the original `updates/{objective_name}.txt` file line-by-line
   - Open and carefully read the `updates/{objective_name}_questions.md` answers section (if Step 5 - second verification round)
   - Open and carefully read your TODO file `updates/todo-files/{objective_name}_todo.md`

2. **Cross-Reference Requirements**:
   - Extract EVERY requirement from the original updates file (explicit and implied)
   - Extract EVERY requirement from the question answers (if Step 5 - second verification round)
   - Compare against TODO file to verify ALL requirements are covered
   - Mark any missing requirements: ‚ùå MISSING FROM TODO
   - During first verification round (Step 2): Identify ambiguities to ask about in questions file

3. **Ask Clarifying Questions to Self**:
   - What specific files will need to be modified for each task?
   - What existing patterns or utilities in the codebase can be leveraged?
   - Are there any edge cases or error scenarios not covered?
   - What test files will need to be created or modified?
   - Are there any dependencies between tasks that need ordering?
   - What existing code should be examined before implementation?

4. **Research Codebase**:
   - Use Glob/Grep to find relevant existing code
   - Look for similar implementations to maintain consistency
   - Identify utility classes or helper functions to reuse
   - Find test file patterns to follow
   - Locate configuration files that may need updates

5. **Update TODO File**:
   - Add missing requirements as new tasks
   - Add specific file paths to each task
   - Add code references (e.g., "Similar to ClassName.method_name in file.py:line")
   - Add test requirements for each implementation task
   - Refine task descriptions with more technical specificity
   - Add prerequisite relationships between tasks

**üìã ITERATION 2: Deep Dive Verification**

1. **Re-read Updated TODO File**:
   - Review all changes made in Iteration 1
   - Check if new questions emerged from codebase research

2. **Ask Additional Clarifying Questions**:
   - What data structures will be passed between functions?
   - What error handling strategies should be used?
   - Are there any performance considerations?
   - What logging should be added for debugging?
   - Should any constants be extracted to configuration files?
   - What documentation needs to be updated?
   - Are there any backward compatibility concerns?

3. **Research Additional Code Patterns**:
   - Find error handling patterns used in similar code
   - Identify logging patterns and conventions
   - Check configuration file structures
   - Review existing documentation formats
   - Look for data validation patterns

4. **Update TODO File Again**:
   - Add error handling tasks
   - Add logging requirements
   - Add documentation update tasks
   - Add data validation requirements
   - Refine implementation order based on dependencies
   - Add code review checkpoints

**üìã ITERATION 3: Deep Dive Verification**

1. **Re-read ALL Documents One More Time**:
   - Original updates file
   - Question answers (if Step 5 - second verification round)
   - Fully refined TODO file (for this round)

2. **Ask Final Technical Questions**:
   - Are there any integration points with other modules not addressed?
   - What mock objects will be needed for testing?
   - Are there any circular dependency risks?
   - What happens if operations fail midway through?
   - Are all file paths absolute or properly constructed?
   - What cleanup operations are needed if errors occur?

3. **Research Integration Points**:
   - Use Grep to find all places existing code is called
   - Identify all modules that might be affected
   - Check for potential circular imports
   - Review existing test mock patterns

4. **TODO Update**:
   - Add integration testing tasks
   - Add cleanup/rollback requirements
   - Add verification steps for each phase
   - Add pre-commit validation checkpoints
   - Ensure every task has specific, actionable steps
   - Confirm task order prevents breaking the system

**üìã ITERATION 4: Enhanced Technical Detail**

1. **Implementation-Specific Research**:
   - Research exact method signatures needed
   - Identify return types and data structures
   - Find example usages of similar patterns in codebase
   - Document edge cases with code examples

2. **Performance and Optimization**:
   - Consider performance implications of approach
   - Identify potential bottlenecks
   - Research caching strategies if needed
   - Document any async/await patterns needed

3. **TODO Enhancement**:
   - Add specific code examples to tasks
   - Document exact API endpoints/parameters
   - Add performance considerations
   - Include optimization opportunities

**üìã ITERATION 5 (First Round) / ITERATION 12 (Second Round): END-TO-END DATA FLOW VERIFICATION**

üö® **CRITICAL**: Trace the complete path from user action to system output.

1. **Identify Entry Points**:
   - List ALL user-facing scripts (run_*.py)
   - List ALL manager classes that orchestrate operations
   - For EACH requirement, identify which entry point triggers it

2. **Trace Data Flow**:
   - For EACH requirement, document the complete path:
     ```
     Entry: run_simulation.py
       ‚Üí SimulationManager.run_iterative_optimization()
       ‚Üí ParallelLeagueRunner.run_simulations_for_config()
       ‚Üí ResultsManager.save_optimal_config()  ‚Üê CURRENT
       ‚Üí ResultsManager.save_optimal_configs_folder()  ‚Üê REQUIRED
     ```
   - Identify WHERE in the flow the new code fits
   - Identify WHAT existing code needs to change

3. **Document Integration Points**:
   - For EACH new method/class, document:
     - What file it will be in
     - What existing method will CALL it
     - What line in the caller needs to change
   - Add these as explicit TODO tasks

4. **Verify No Orphan Code**:
   - Review each new component planned
   - Confirm each has a caller identified
   - If any new code has no caller ‚Üí add integration task

**üìã ITERATION 6 (First Round) / ITERATION 13 (Second Round): SKEPTICAL RE-VERIFICATION**

üö® **CRITICAL**: Assume NOTHING written so far is accurate. Start fresh.

1. **Question Everything**:
   - Is the file path I documented actually correct? (Verify with Read/Glob)
   - Does that method I referenced actually exist? (Verify with Grep)
   - Is that pattern I identified actually used? (Re-search codebase)
   - Are my assumptions about data flow correct? (Re-trace through code)
   - Did I misunderstand the original requirement? (Re-read specification)

2. **Fresh Codebase Validation**:
   - Re-search for ALL file paths mentioned in TODO
   - Re-verify ALL method names and line numbers
   - Re-validate ALL code patterns claimed
   - Re-check ALL dependencies and imports
   - Re-examine ALL test patterns documented

3. **Requirement Re-Verification**:
   - Re-read original specification word-by-word
   - Re-read user question answers (if second round)
   - List requirements again from scratch
   - Compare new list against TODO
   - Identify discrepancies or misinterpretations

4. **Comprehensive Corrections**:
   - Fix any incorrect file paths
   - Correct any wrong method names
   - Update any misunderstood patterns
   - Revise any flawed implementation strategies
   - **Add missing integration tasks** (caller modifications)
   - Document what was corrected and why

5. **Document Re-Verification**:
   - Add "Skeptical Re-Verification Results" section to TODO
   - List what was verified as correct
   - List what was found to be incorrect and corrected
   - Document confidence level in current plan

**üìã ITERATION 7 (First Round) / ITERATION 14 (Second Round): INTEGRATION GAP CHECK**

üö® **CRITICAL**: Final check that all new code will actually be used.

1. **Create Integration Matrix**:
   For EACH new component, fill out:
   ```
   | New Component | File | Called By | Caller File:Line | Caller Modification Task |
   |---------------|------|-----------|------------------|--------------------------|
   | save_optimal_configs_folder() | ResultsManager.py | run_iterative_optimization() | SimulationManager.py:261 | Task 4.2 |
   ```

2. **Verify Caller Modifications in TODO**:
   - For each row in the matrix, confirm a TODO task exists to modify the caller
   - If missing ‚Üí add the task immediately
   - Flag with ‚ö†Ô∏è if integration was previously overlooked

3. **Check for Orphan Code**:
   - Any new component without a caller = orphan code
   - Either add caller modification task OR remove the orphan component
   - Document decision in TODO

4. **Verify Entry Point Coverage**:
   - For each entry point affected by requirements
   - Trace through to output
   - Confirm all new code is in the execution path

5. **üö® Check Entry Script File Discovery Patterns** (NEW):
   - If output format changes (e.g., JSON ‚Üí folder, single file ‚Üí multiple files):
     - List all `run_*.py` scripts that auto-detect or find these files
     - Verify glob patterns match new output format
     - Verify file validation logic matches new structure
     - Add TODO tasks for any entry script updates needed
   - Create Entry Script Update Matrix:
     ```
     | Output Change | Entry Script | Discovery Pattern | Needs Update? | TODO Task |
     |---------------|--------------|-------------------|---------------|-----------|
     | JSON ‚Üí folder | run_simulation.py | glob("optimal_*.json") | YES | Task 5.1 |
     ```

**üìã ITERATIONS 8-11 (Second Round): Standard Verification with Answers**

Continue with standard verification process (same as Iterations 1-4) but with user answers integrated.

**üìã ITERATIONS 15-16 (Second Round): Final Preparation**

1. **Final Integration Check**:
   - Verify all user answers are reflected in implementation plan
   - Cross-check dependencies one final time
   - Validate testing strategy is comprehensive
   - Confirm documentation updates are complete

2. **Implementation Readiness**:
   - Ensure every task is actionable and clear
   - Verify no ambiguities remain
   - Confirm all code references are accurate
   - Validate execution order is optimal

3. **Create Integration Verification Checklist**:
   - List every new method/class with its caller
   - Create a checklist to verify during implementation:
     ```
     ‚ñ° New method created: save_optimal_configs_folder()
     ‚ñ° Caller modified: SimulationManager.py:261
     ‚ñ° Entry point tested: python run_simulation.py --mode iterative
     ‚ñ° Output verified: folder with 4 files created
     ```

4. **Risk Assessment**:
   - Document remaining risks
   - Identify potential blockers
   - Plan mitigation strategies
   - Set success criteria

### **DOCUMENTATION REQUIREMENTS**:

After completing each verification round, add/update a "Verification Summary" section to the TODO file documenting:
- ‚úÖ Number of iterations completed (7 for first round, 16 total after second round)
- ‚úÖ Number of requirements added after initial draft
- ‚úÖ Key codebase patterns/utilities identified for reuse
- ‚úÖ Critical dependencies or ordering requirements
- ‚úÖ Risk areas identified during research
- ‚úÖ Questions identified for user clarification (first round)
- ‚úÖ Question answers integrated into plan (second round)
- ‚úÖ **Data flow traces** for each requirement (entry point ‚Üí output)
- ‚úÖ **Integration matrix** showing new components and their callers
- ‚úÖ **Skeptical re-verification results** (corrections made, confidence level)

### **ACCEPTANCE CRITERIA** (before beginning implementation):
- ‚úÖ First verification round complete (7 iterations on draft TODO, including data flow, skeptical re-verification, and integration gap check)
- ‚úÖ Questions file created with thoughtful, research-backed questions
- ‚úÖ User answers received for all questions
- ‚úÖ Second verification round complete (9 more iterations with answers integrated, including second data flow, skeptical re-verification, and integration gap check)
- ‚úÖ **Total: 16 complete verification iterations performed** (with 2 data flow verifications, 2 skeptical re-verifications, 2 integration gap checks)
- ‚úÖ Every requirement from original file covered in TODO
- ‚úÖ Every question answer reflected in TODO tasks
- ‚úÖ Specific file paths identified for each task (verified multiple times)
- ‚úÖ Existing code patterns researched and documented (validated in skeptical phases)
- ‚úÖ Test requirements specified for each implementation
- ‚úÖ Task dependencies and ordering verified
- ‚úÖ Edge cases and error scenarios addressed
- ‚úÖ Documentation update tasks included
- ‚úÖ Pre-commit validation checkpoints added
- ‚úÖ **All claims and assumptions verified through skeptical re-verification**
- ‚úÖ **Data flow traced from entry point to output for each requirement**
- ‚úÖ **Integration matrix created showing all new components and their callers**
- ‚úÖ **Caller modifications included in TODO (not just new method creation)**
- ‚úÖ **Integration verification checklist created for implementation phase**

### **ENFORCEMENT**:
- **NO SHORTCUTS**: Cannot skip iterations or rush through verification
- **TWO ROUNDS REQUIRED**: Must complete both verification rounds (7 iterations + 9 iterations = 16 total)
- **DATA FLOW PHASES MANDATORY**: Must complete iterations 5 and 12 as end-to-end data flow verification
- **SKEPTICAL PHASES MANDATORY**: Must complete iterations 6 and 13 as skeptical re-verifications
- **INTEGRATION GAP PHASES MANDATORY**: Must complete iterations 7 and 14 as integration gap checks
- **QUESTIONS REQUIRED**: Must create questions file after first verification round
- **NO ASSUMPTIONS**: Must research codebase to validate approach
- **NO VAGUE TASKS**: Each task must have specific technical details
- **ITERATIVE IS MANDATORY**: Minimum 16 total cycles of read-question-research-update
- **DOCUMENT THE PROCESS**: Verification summary required in TODO file after each round
- **FRESH EYES REQUIRED**: Skeptical iterations must approach verification as if seeing the plan for the first time
- **INTEGRATION MATRIX REQUIRED**: Must create matrix showing new components and their callers

### **WHY THIS MATTERS**:
Thorough TODO preparation with skeptical re-verification prevents:
- ‚ùå Discovering missing requirements mid-implementation
- ‚ùå Breaking existing functionality due to unforeseen dependencies
- ‚ùå Inconsistent code that doesn't match project patterns
- ‚ùå Incomplete implementations that miss edge cases
- ‚ùå Failed tests due to inadequate test planning
- ‚ùå Rework and refactoring caused by poor initial planning
- ‚ùå **Incorrect assumptions about code structure or API behavior**
- ‚ùå **Pursuing implementation strategies based on misunderstood patterns**
- ‚ùå **Building on top of incorrect file paths or method references**
- ‚ùå **Creating infrastructure methods that are never called from entry points**
- ‚ùå **Building components without wiring them into the actual system**
- ‚ùå **Changing output formats without updating entry script file discovery patterns**

**The specialized verification phases are critical**:

**Data Flow Verification (iterations 5 and 12)** forces the agent to:
1. Trace the complete path from user action to system output
2. Identify where new code fits in the execution flow
3. Document what existing code needs to change to use new code
4. Prevent orphan code that is never called

**Skeptical Re-Verification (iterations 6 and 13)** forces the agent to:
1. Challenge its own assumptions
2. Re-validate all claims with fresh codebase research
3. Catch errors that might have been overlooked in earlier iterations
4. Ensure the plan is built on accurate, verified information

**Integration Gap Check (iterations 7 and 14)** forces the agent to:
1. Create a matrix of new components and their callers
2. Verify every new method has a corresponding caller modification task
3. Flag and fix any orphan code before implementation begins
4. Ensure the TODO includes both infrastructure AND integration tasks

A well-researched, thoroughly verified, and integration-validated TODO file is the foundation of successful implementation.
Immediately after creating the TODO file, create a code changes documentation file in the updates folder (NOT the done folder yet) named `{objective_name}_code_changes.md`. This file should be updated incrementally as you work through each task in the TODO file. After completing each significant change, immediately document it in the code changes file with: file paths, line numbers, before/after code snippets, rationale, and impact. This ensures the documentation stays current and accurate throughout the implementation process. The file will be moved to updates/done when the objective is complete.
After updates are complete, verify that all unit tests still pass and test the system to ensure it is functional. Test ALL unit tests across the entire repo. All tests across the entire repo must pass before we can mark an objective as complete. Make this a very clear and important TODO item.
Create any new unit tests to cover the new implementations, and modify any relevant unit tests and test them to ensure they pass.
Update rules files and readme files according to the new changes.
When the objective is complete, ensure the code changes documentation file is comprehensive and finalized. It should detail all code modifications made during the implementation, including: specific file paths and line numbers, before/after code snippets, rationale for each change, impact analysis, configuration changes, test modifications, and verification of files that were checked but not modified. This serves as a complete technical reference for understanding exactly what changed and why. Move this file from updates/ to updates/done/ along with the objective file.
Once the objective is completely done, then move the associated file in updates to the 'done' folder contained within the same updates directory. Also delete the questions file once the objective is complete.
Be VERY systematic when creating the TODO file and proceeding with the implementation. Break the objective up into modualized components. Each phase that is outlined in the todo file should leave the repo is a state that is still testable and functional, and should run the pre-commit checklist in its entirety. Do not skip any unit tests or interactive integration tests just because you think that the tests will not be effected by the change - assume everything is at risk of breaking no matter what you do.

## END-TO-END DATA FLOW VERIFICATION PROTOCOL

**üö® CRITICAL: INFRASTRUCTURE vs INTEGRATION** - Creating helper methods, utility functions, or infrastructure components is NOT the same as integrating them into the system. This protocol ensures new code is actually USED, not just created.

### **THE INFRASTRUCTURE TRAP**:
A common failure pattern is:
1. ‚úÖ Create new methods/classes that implement required functionality
2. ‚úÖ Write tests for those new methods
3. ‚úÖ Tests pass
4. ‚ùå **BUT**: The new methods are never called from entry points
5. ‚ùå **RESULT**: Feature appears complete but doesn't actually work for users

**Example of this failure**:
- Requirement: "Simulation should output folders instead of single files"
- What was done: Created `save_optimal_configs_folder()` method with tests
- What was missed: `SimulationManager.run_iterative_optimization()` still calls `save_optimal_config()` (single file)
- Result: Method exists, tests pass, but simulation still outputs single files

### **WHEN TO EXECUTE**:
- ‚úÖ During TODO file creation (identify entry points that need modification)
- ‚úÖ During skeptical re-verification (trace data flow for each requirement)
- ‚úÖ After implementation (verify new code is called from entry points)
- ‚úÖ Before marking objective complete (final integration check)

### **MANDATORY VERIFICATION STEPS**:

**üìã STEP 1: Identify ALL Entry Points**

Entry points are user-facing scripts or functions where execution begins:
- Root scripts: `run_*.py` files
- Main functions: `if __name__ == "__main__"` blocks
- Manager classes: Top-level orchestrators (e.g., `SimulationManager`, `LeagueHelperManager`)
- CLI commands: User-invoked operations

For each requirement, ask:
- "What script/function does the user run to trigger this?"
- "What is the entry point for this feature?"

**üìã STEP 2: Trace Data Flow from Entry to Output**

For each requirement, trace the COMPLETE path:
```
Entry Point ‚Üí Manager/Controller ‚Üí Service/Helper ‚Üí Output
```

Document this flow in the TODO file:
```
Requirement: "Simulation outputs folders"
Entry Point: run_simulation.py ‚Üí SimulationManager.run_iterative_optimization()
Current Flow: ... ‚Üí ResultsManager.save_optimal_config() ‚Üí single file
Required Flow: ... ‚Üí ResultsManager.save_optimal_configs_folder() ‚Üí folder with 4 files
Files to Modify: SimulationManager.py (change method call)
```

**üìã STEP 3: Verify Integration Points**

For each NEW method/class created, answer:
- "What existing code will CALL this new code?"
- "Is there a modification needed in the caller?"
- "Have I added the call, or just created the method?"

Create an integration checklist:
```
New Method: ResultsManager.save_optimal_configs_folder()
Called By: SimulationManager.run_iterative_optimization() ‚Üê NEEDS MODIFICATION
Current Call: save_optimal_config()
New Call: save_optimal_configs_folder()
Status: ‚ùå NOT INTEGRATED (method exists but not called)
```

**üìã STEP 4: Test from Entry Point**

After implementation, verify by starting from the entry point:
1. Run the actual user-facing script
2. Verify the output matches requirements
3. Don't just run unit tests - run the actual feature

```bash
# Don't just run: pytest tests/simulation/test_ResultsManager.py
# Also run: python run_simulation.py --mode iterative
# Then verify: ls simulation/optimal_configs/  # Should show folders, not files
```

### **INTEGRATION CHECKLIST TEMPLATE**:

Add this to TODO file for each new component:

```markdown
## Integration Verification

### New Component: [ClassName.method_name()]
- [ ] Method implemented
- [ ] Unit tests pass
- [ ] Called from: [CallerClass.caller_method()]
- [ ] Caller modified to use new method
- [ ] Entry point tested: [script_name.py]
- [ ] Output verified: [expected output description]

### Data Flow Trace:
Entry: [entry_point]
  ‚Üí [step 1]
  ‚Üí [step 2]
  ‚Üí [NEW: step using new component]
  ‚Üí Output: [expected output]
```

### **RED FLAGS TO WATCH FOR**:

üö© "I created a new method but didn't modify any existing code to call it"
üö© "The tests pass but I haven't run the actual script"
üö© "I built the infrastructure but the manager/controller still uses the old approach"
üö© "I added capability but didn't wire it into the execution path"
üö© "I changed the output format but entry scripts still look for the old format"
üö© "I changed file structure but CLI auto-detection still uses old patterns"

### **ENTRY POINT FILE DISCOVERY CHECKLIST**:

üö® **CRITICAL**: When output formats change (e.g., single file ‚Üí folder, JSON ‚Üí CSV), entry point scripts often have file discovery logic that must be updated.

**Common Entry Script Patterns That Need Updating**:
1. **Auto-detection patterns**: `glob("optimal_*.json")` ‚Üí `glob("optimal_*/")` with `.is_dir()` check
2. **File validation logic**: Checking for `.json` extension ‚Üí checking for folder with required files
3. **Path resolution**: Loading single file ‚Üí loading folder with multiple files
4. **Error messages**: "No .json files found" ‚Üí "No config folders found"
5. **Help text and documentation**: Describing old format ‚Üí describing new format

**When Output Format Changes, Check These Files**:
- `run_*.py` - All CLI entry scripts
- Argument parser help text
- Default path constants
- File/folder discovery functions
- Validation and error handling
- README and documentation examples

**Verification Template**:
```
Output Format Change: Single JSON ‚Üí Folder with 4 files
Files Creating Output: ResultsManager.save_optimal_configs_folder()
Entry Scripts Affected: run_simulation.py
Discovery Pattern Change: glob("optimal_*.json") ‚Üí glob("optimal_*/") + folder validation
Error Message Update: "No optimal config files" ‚Üí "No optimal config folders"
Help Text Update: --baseline accepts folder path, not JSON file
‚úÖ All entry script patterns updated to match new output format
```

### **ENFORCEMENT**:
- **NO ORPHAN CODE**: Every new method must have an identified caller
- **TRACE REQUIRED**: Data flow must be documented from entry point to output
- **INTEGRATION MANDATORY**: Creating infrastructure without integration = incomplete
- **END-TO-END TEST**: Must verify from user entry point, not just unit tests

---

## REQUIREMENT VERIFICATION PROTOCOL

**üö® CRITICAL: FINAL VERIFICATION BEFORE MARKING OBJECTIVE COMPLETE** - This protocol must be executed before claiming an objective is complete:

### **WHEN TO EXECUTE**:
- ‚úÖ After all implementation work appears to be complete
- ‚úÖ Before moving objective files to updates/done/
- ‚úÖ Before deleting questions files
- ‚úÖ Before claiming "objective complete" to the user

### **MANDATORY VERIFICATION STEPS**:

**üìã STEP 1: Re-read Original Requirements File**
- Open and read EVERY LINE of the original `updates/{objective_name}.txt` file
- Create a checklist of EVERY requirement mentioned (numbered or implied)
- For each requirement, verify it has been implemented
- Mark each requirement as ‚úÖ DONE or ‚ùå MISSING

**üìã STEP 2: Re-read Question Answers File**
- Open and read EVERY ANSWER in `updates/{objective_name}_questions.md`
- Create a checklist of EVERY answer that implies implementation work
- For each answer, verify the implementation matches what was answered
- Mark each answer as ‚úÖ IMPLEMENTED or ‚ùå NOT IMPLEMENTED

**üìã STEP 3: Search Codebase for Implementation**
- Use Grep/Glob tools to search for evidence of each requirement
- Verify files were actually modified as required
- Check that new files were created if specified
- Confirm no placeholder code or TODOs remain

**üìã STEP 4: Verify End-to-End Integration**
- For each NEW method/function created:
  - Search codebase to find what CALLS this method
  - If nothing calls it ‚Üí ‚ùå INTEGRATION MISSING
  - Verify the caller is in the execution path from entry point
- For each requirement:
  - Trace from entry point (run_*.py) to output
  - Verify the new code is in the execution path
  - Run the actual script to confirm behavior (not just unit tests)
- Create integration evidence:
  ```
  Requirement: "Simulation outputs folders"
  New Method: save_optimal_configs_folder()
  Called By: SimulationManager.run_iterative_optimization() line 261
  Entry Point: run_simulation.py --mode iterative
  Verified: ‚úÖ Running script produces folder output
  ```

**üìã STEP 5: Identify Missing Requirements**
- If ANY requirement is ‚ùå MISSING or ‚ùå NOT IMPLEMENTED or ‚ùå INTEGRATION MISSING:
  - STOP immediately
  - Create a new completion TODO file: `updates/todo-files/{objective_name}_completion_todo.md`
  - Document ALL missing requirements and integration gaps
  - Implement missing requirements before proceeding
  - Re-run this verification protocol

**üìã STEP 6: Document Verification**
- Add a "Requirements Verification" section to the code changes file
- List each requirement and its implementation status
- Include file paths and line numbers as evidence
- Confirm 100% requirement coverage

### **ACCEPTANCE CRITERIA**:
- ‚úÖ ALL requirements from original file implemented (100% coverage)
- ‚úÖ ALL question answers reflected in implementation
- ‚úÖ NO missing functionality or partial implementations
- ‚úÖ NO half-measures or "mostly complete" status
- ‚úÖ Evidence of implementation exists in codebase (files, code, tests)
- ‚úÖ All unit tests pass (100%)
- ‚úÖ Manual testing confirms functionality works
- ‚úÖ **ALL new methods have identified callers (no orphan code)**
- ‚úÖ **Integration verified from entry point to output**
- ‚úÖ **Actual scripts run and produce expected behavior (not just unit tests)**

### **ENFORCEMENT**:
- **NO EXCEPTIONS**: If even ONE requirement is missing, objective is NOT complete
- **NO PARTIAL CREDIT**: "90% complete" = incomplete, must finish remaining 10%
- **NO ASSUMPTIONS**: Just because something seems done doesn't mean it is - verify with code
- **RE-VERIFICATION REQUIRED**: If new requirements discovered, re-run entire protocol
- **NO ORPHAN CODE**: Methods that exist but are never called = incomplete implementation
- **INTEGRATION REQUIRED**: Infrastructure without integration = feature doesn't work

### **FAILURE TO VERIFY = INCOMPLETE OBJECTIVE**:
If this protocol is not executed, or if missing requirements are discovered after claiming completion, the objective must be reopened and completed properly. The user must be notified of any missing requirements immediately.

**Common Integration Failures to Check**:
- ‚ùå Created helper method but manager still uses old method
- ‚ùå Added new output format but save function still uses old format
- ‚ùå Built infrastructure but controller not updated to use it
- ‚ùå Tests pass but running actual script shows old behavior
- ‚ùå Changed output format (JSON ‚Üí folder) but entry script still looks for old format
- ‚ùå Entry script glob patterns don't match new file/folder structure
- ‚ùå Error messages reference old format instead of new format

## PRE-COMMIT VALIDATION PROTOCOL

**üö® MANDATORY FOR EVERY STAGE COMPLETION** - This protocol must be executed at the completion of EVERY phase, step, or significant change before proceeding to the next stage:

### **WHEN TO EXECUTE**:
- ‚úÖ After completing ANY phase step (e.g., Phase 2 Step 2.1)
- ‚úÖ Before moving to the next phase or step
- ‚úÖ When instructed to "validate and commit" or "commit changes"
- ‚úÖ At any major milestone or completion point
- ‚úÖ Before asking user for validation to proceed

### **MANDATORY EXECUTION STEPS**:

**üöÄ REQUIRED: Run Unit Tests**

Run the unit test suite to validate all changes:

```bash
python tests/run_all_tests.py
```

This command executes ALL unit tests:
- ‚úÖ Complete unit test suite in tests/ directory
- ‚úÖ Strict 100% pass requirement
- ‚úÖ Returns exit code 0 for success, 1 for failure

**Exit Codes**:
- `0` = All tests passed (100%), safe to commit
- `1` = Some tests failed, DO NOT COMMIT until issues are fixed

**Test Options**:
```bash
python tests/run_all_tests.py --verbose    # Show individual test names
python tests/run_all_tests.py --detailed   # Full test output
python tests/run_all_tests.py --single     # Faster single command mode
```

**Validation Steps**:

1. **ANALYZE CHANGES**:
   ```bash
   git status
   git diff
   ```
   - Review ALL changed files
   - Understand impact of changes

2. **ADD/UPDATE UNIT TESTS**:
   - Add tests for new functionality in `tests/` directory
   - Follow test structure: `tests/module_path/test_FileName.py`
   - Use proper mocking to isolate functionality
   - Ensure tests follow Arrange-Act-Assert pattern

3. **RUN ALL UNIT TESTS** (MANDATORY):
   ```bash
   python tests/run_all_tests.py
   ```
   - **100% pass rate required**
   - Fix any failing tests before proceeding
   - Re-run until all tests pass

4. **MANUAL TESTING** (if applicable):
   - Test the affected functionality manually
   - Run the main scripts to verify behavior:
     ```bash
     python run_league_helper.py   # League helper mode
     python run_player_fetcher.py  # Player data fetcher
     python run_simulation.py      # Simulation system
     ```

5. **UPDATE DOCUMENTATION**:
   - Update README.md if functionality changed
   - Update CLAUDE.md if workflow or architecture changed
   - Update rules.txt if development process changed
   - Update module-specific documentation as needed

6. **COMMIT STANDARDS**:
   - Format: "Brief description of change"
   - Keep under 50 characters when possible
   - NO emojis or icons
   - Do NOT include "Generated with Claude Code" footer
   - List major changes in commit body if needed

### **FAILURE PROTOCOL**:
- **If ANY test fails**: STOP immediately, fix the issue, re-run tests
- **If unit tests fail**: Fix failing tests, ensure 100% pass rate before proceeding
- **No exceptions**: Cannot proceed to next phase without 100% test success
- **Cannot commit**: Do NOT commit code with failing tests

### **ENFORCEMENT**:
- This protocol is MANDATORY and NON-NEGOTIABLE for every stage completion
- Violation of this protocol requires immediate correction and re-validation
- The user should be notified if this protocol was not followed properly
